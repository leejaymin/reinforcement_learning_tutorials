{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # progressbar \n",
    "import time\n",
    "import copy # copyë¥¼ ì‹¤ìˆ˜ í•˜ì§€ ì•Šê²Œ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë¦¼ê·¸ë¦¬ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table ê·¸ë¦¬ê¸°  (ìƒíƒœì˜ ê°€ì¹˜ ì´ 9ê°œ 3x3)\n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table ê·¸ë¦¬ê¸° (3x3x4) ì—‘ì…˜ ê°€ì¹˜ \n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        â†‘       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      â†  â†’     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          â†’     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      â†         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        â†“       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   â†‘     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   â†’     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   â†“     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   â†     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# ì •ì±… policy í™”ì‚´í‘œë¡œ ê·¸ë¦¬ê¸°\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      â†‘         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      â†’         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      â†“         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      â†         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ì£¼ì²´ \n",
    "class Agent():\n",
    "    \n",
    "    # 1. í–‰ë™ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ì˜ ì¢Œí‘œ ì´ë™(ìœ„, ì˜¤ë¥¸ìª½, ì•„ë˜, ì™¼ìª½) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. ê° í–‰ë™ë³„ ì„ íƒí™•ë¥ \n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ìœ„ì¹˜ ì €ì¥\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì €ì¥\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Agent_jemin():\n",
    "\n",
    "    # 1. í–‰ë™ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ì˜ ì¢Œí‘œ ì´ë™(ìœ„, ì˜¤ë¥¸ìª½, ì•„ë˜, ì™¼ìª½)\n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "\n",
    "    # 2. ê° í–‰ë™ë³„ ì„ íƒí™•ë¥ \n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "\n",
    "    # 3. ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ìœ„ì¹˜ ì €ì¥\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "\n",
    "    # 4. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì €ì¥\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "\n",
    "    # 5. ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    def get_pos(self):\n",
    "        return self.pos\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Environment_jemin(): #í™˜ê²½ ì •ì˜ë„ ì¤‘ìš”í•˜ë‹¤.\n",
    "\n",
    "    # 1. ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "\n",
    "    # 2. ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •\n",
    "    goal_position = [4,4]\n",
    "\n",
    "    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì\n",
    "    reward_list = [[road,road,road,road,road],\n",
    "                   [road,road,road,road,road],\n",
    "                   [cliff,cliff,road,cliff,cliff],\n",
    "                   [road,road,road,road,road],\n",
    "                   [road,road,road,road,goal]]\n",
    "\n",
    "    # 4. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\",\"road\",\"road\"],\n",
    "                    [\"cliff\",\"cliff\",\"road\",\"cliff\",\"cliff\"],\n",
    "                    [\"road\",\"road\",\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\",\"road\",\"goal\"]]\n",
    "\n",
    "    # 5. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)\n",
    "\n",
    "    # 6. ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)\n",
    "    def move(self, agent, action):\n",
    "\n",
    "        done = False\n",
    "\n",
    "        # 6.1 í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "\n",
    "        # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸ and cliff ì¸ì§€ í™•ì¸\n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1] :\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        elif self.reward_list1[new_pos[0]][new_pos[1]] == \"cliff\":\n",
    "            #print(\"cliff\")\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "\n",
    "        return observation, reward, done\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(): #í™˜ê²½ ì •ì˜ë„ ì¤‘ìš”í•˜ë‹¤. \n",
    "    \n",
    "    # 1. ë¯¸ë¡œë°–(ì ˆë²½), ê¸¸, ëª©ì ì§€ì™€ ë³´ìƒ ì„¤ì •\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ìˆ«ì\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ ë¬¸ì\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. ë³´ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ì„¤ì •\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. ì„ íƒëœ ì—ì´ì „íŠ¸ì˜ í–‰ë™ ê²°ê³¼ ë°˜í™˜ (ë¯¸ë¡œë°–ì¼ ê²½ìš° ì´ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ë³µê·€)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 í–‰ë™ì— ë”°ë¥¸ ì¢Œí‘œ êµ¬í•˜ê¸°\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 í˜„ì¬ì¢Œí‘œê°€ ëª©ì ì§€ ì¸ì§€í™•ì¸\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 ì´ë™ í›„ ì¢Œí‘œê°€ ë¯¸ë¡œ ë°–ì¸ í™•ì¸    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 ì´ë™ í›„ ì¢Œí‘œê°€ ê¸¸ì´ë¼ë©´\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¬ê·€ì ìœ¼ë¡œ ìƒíƒœ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ ê°€ì¹˜ ê³„ì‚°\n",
    "def state_value_function(env,agent,G,max_step,now_step):\n",
    "    \n",
    "    # 1. ê°ê°€ìœ¨ ì„¤ì •\n",
    "    gamma = 0.9\n",
    "    \n",
    "# 2. í˜„ì¬ ìœ„ì¹˜ê°€ ë„ì°©ì§€ì ì¸ì§€ í™•ì¸\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    \n",
    "# 3. ë§ˆì§€ë§‰ ìƒíƒœëŠ” ë³´ìƒë§Œ ê³„ì‚°\n",
    "    if (max_step == now_step):\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 3.1 ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì˜ ë³´ìƒì„ ê³„ì‚°\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            observation, reward, done = env.move(agent,i)\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "            \n",
    "        return G # ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°’ë§Œ ë¦¬í„´í•œë‹¤.\n",
    "    \n",
    "    # 4. í˜„ì¬ ìƒíƒœì˜ ë³´ìƒì„ ê³„ì‚°í•œ í›„ ë‹¤ìŒ stepìœ¼ë¡œ ì´ë™\n",
    "    else:\n",
    "        \n",
    "        # 4.1í˜„ì¬ ìœ„ì¹˜ ì €ì¥\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.2 í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì„ ì¡°ì‚¬í•œ í›„ ì´ë™\n",
    "        for i in range(len(agent.action)):\n",
    "            observation, reward, done = env.move(agent,i)      \n",
    "            # 4.2.1 í˜„ì¬ ìƒíƒœì—ì„œ ë³´ìƒì„ ê³„ì‚°\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "\n",
    "            # 4.2.2 ì´ë™ í›„ ìœ„ì¹˜ í™•ì¸ : ë¯¸ë¡œë°–, ë²½, êµ¬ë©ì¸ ê²½ìš° ì´ë™ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ì´ë™\n",
    "            if done == True:\n",
    "                if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                    agent.set_pos(pos1)\n",
    "\n",
    "            # 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\n",
    "            # recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\n",
    "            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v # ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\n",
    "\n",
    "            # 4.2.4 í˜„ì¬ ìœ„ì¹˜ë¥¼ ë³µêµ¬\n",
    "            agent.set_pos(pos1)\n",
    "\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¯¸ë¡œì˜ ê° ìƒíƒœì˜ ìƒíƒœê°€ì¹˜í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 0 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.50      |      -2.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -1.50      |      -1.00      |      -1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.00      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.58      |      -2.96      |      -3.46      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.96      |      -2.12      |      -1.68      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.46      |      -1.68      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 2 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.94      |      -4.23      |      -4.60      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.23      |      -3.09      |      -2.41      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.60      |      -2.41      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 3 total_time = 0.02(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.13      |      -5.29      |      -5.56      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.29      |      -3.99      |      -3.05      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.56      |      -3.05      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 4 total_time = 0.06(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.14      |      -6.22      |      -6.38      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.22      |      -4.75      |      -3.61      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.38      |      -3.61      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 5 total_time = 0.19(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.01      |      -7.01      |      -7.08      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.01      |      -5.42      |      -4.09      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.08      |      -4.09      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 6 total_time = 0.63(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.76      |      -7.69      |      -7.69      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -6.00      |      -4.51      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -4.51      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-a4c50312716e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreward\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m             \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_pos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mj\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m             \u001B[0mv_table\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mj\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;31m# 5.3 max_downì— ë”°ë¥¸ ê³„ì‚°ì‹œê°„ ì €ì¥\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 ë‹¤ìŒ stepì„ ê³„ì‚°\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. ëë‚˜ëŠ” ì¡°ê±´ì€ ê³¨ì— ë„ë‹¬í•˜ê±°ë‚˜ ëì¼ ë•Œì´ë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# ì·¨í•œ ì—‘ì…˜, ë””ìŠ¤ì¹´ìš´íŠ¸ íŒ©í„°, ë‹¤ìŒ ì—‘ì…˜\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     17\u001B[0m             \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_pos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpos1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m             \u001B[0mobservation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmove\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mG\u001B[0m \u001B[1;31m# ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°’ë§Œ ë¦¬í„´í•œë‹¤.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 1. í™˜ê²½ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "\n",
    "# 2. ì—ì´ì „íŠ¸ ì´ˆê¸°í™”\n",
    "agent = Agent()\n",
    "\n",
    "# 3. ìµœëŒ€ max_step_number ì œí•œ\n",
    "max_step_number = 13\n",
    "\n",
    "# 4. ê³„ì‚° ì‹œê°„ ì €ì¥ì„ ìœ„í•œ list\n",
    "time_len = []\n",
    "\n",
    "# 5. ì¬ê·€í•¨ìˆ˜ state_value_functionì„ë¥¼ ì´ìš©í•´ ê° ìƒíƒœ ê°€ì¹˜ë¥¼ ê³„ì‚°\n",
    "for max_step in range(max_step_number):\n",
    "    \n",
    "    # 5.1 ë¯¸ë¡œ ê° ìƒíƒœì˜ ê°€ì¹˜ë¥¼ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "    v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 5.2 ë¯¸ë¡œì˜ ê° ìƒíƒœì— ëŒ€í•´ state_value_function() ì„ ì´ìš©í•´ ê°€ì¹˜ë¥¼ ê³„ì‚°í•œ í›„ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            agent.set_pos([i,j])\n",
    "            v_table[i,j] = state_value_function(env,agent, 0, max_step, 0)\n",
    "            \n",
    "    # 5.3 max_downì— ë”°ë¥¸ ê³„ì‚°ì‹œê°„ ì €ì¥\n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n",
    "    \n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 6. step ë³„ ê³„ì‚° ì‹œê°„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°    \n",
    "plt.plot(time_len, 'o-k')\n",
    "plt.xlabel('max_down')\n",
    "plt.ylabel('time(s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¬ê·€ì ìœ¼ë¡œ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜\n",
    "def action_value_function(env, agent, act, G, max_step, now_step):   \n",
    "    \n",
    "    # 1. ê°ê°€ìœ¨ ì„¤ì •\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 2. í˜„ì¬ ìœ„ì¹˜ê°€ ëª©ì ì§€ì¸ì§€ í™•ì¸\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "\n",
    "    # 3. ë§ˆì§€ë§‰ ìƒíƒœëŠ” ë³´ìƒë§Œ ê³„ì‚°\n",
    "    if (max_step == now_step):\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act]*reward\n",
    "        return G\n",
    "    \n",
    "    # 4. í˜„ì¬ ìƒíƒœì˜ ë³´ìƒì„ ê³„ì‚°í•œ í›„ ë‹¤ìŒ í–‰ë™ê³¼ í•¨ê»˜ ë‹¤ìŒ stepìœ¼ë¡œ ì´ë™\n",
    "    else:\n",
    "        # 4.1í˜„ì¬ ìœ„ì¹˜ ì €ì¥\n",
    "        pos1 = agent.get_pos()\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act] * reward\n",
    "        \n",
    "        # 4.2 ì´ë™ í›„ ìœ„ì¹˜ í™•ì¸ : ë¯¸ë¡œë°–, ë²½, êµ¬ë©ì¸ ê²½ìš° ì´ë™ì „ ì¢Œí‘œë¡œ ë‹¤ì‹œ ì´ë™\n",
    "        if done == True:            \n",
    "            if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                agent.set_pos(pos1)\n",
    "            \n",
    "        # 4.3 í˜„ì¬ ìœ„ì¹˜ë¥¼ ë‹¤ì‹œ ì €ì¥\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.4 í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì„ ì„ íƒí•œ í›„ ì´ë™\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¯¸ë¡œì˜ ê° ìƒíƒœì˜ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ì¬ê·€ì ìœ¼ë¡œ í–‰ë™ì˜ ê°€ì¹˜ë¥¼ ê³„ì‚°\n",
    "\n",
    "# 1. í™˜ê²½ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "\n",
    "# 2. ì—ì´ì „íŠ¸ ì´ˆê¸°í™”\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "# 3. í˜„ì¬ë¶€í„° max_step ê¹Œì§€ ê³„ì‚°\n",
    "max_step_number = 8\n",
    "\n",
    "# 4. ëª¨ë“  ìƒíƒœì— ëŒ€í•´\n",
    "for max_step in range(max_step_number):\n",
    "    # 4.1 ë¯¸ë¡œ ìƒì˜ ëª¨ë“  ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í–‰ë™ì˜ ê°€ì¹˜ë¥¼ ì €ì¥í•  í…Œì´ë¸”ì„ ì •ì˜\n",
    "    print(\"max_step = {}\".format(max_step))\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            # 4.2 ëª¨ë“  í–‰ë™ì— ëŒ€í•´\n",
    "            for action in range(len(agent.action)):\n",
    "                # 4.2.1 ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ë¥¼ ì´ˆê¸°í™”\n",
    "                agent.set_pos([i,j])\n",
    "                # 4.2.2 í˜„ì¬ ìœ„ì¹˜ì—ì„œ í–‰ë™ ê°€ì¹˜ë¥¼ ê³„ì‚°\n",
    "                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n",
    "\n",
    "    q = np.round(q_table,2)\n",
    "    print(\"Q - table\")\n",
    "    show_q_table(q, env)\n",
    "    print(\"High actions Arrow\")\n",
    "    show_q_table_arrow(q,env)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°˜ë³µ ì •ì±… í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ë°˜ë³µ ì •ì±… í‰ê°€\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# 1. ëª¨ë“  ğ‘ âˆˆğ‘†^ì— ëŒ€í•´ì„œ ë°°ì—´ ğ‘‰(ğ‘ )=0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "\n",
    "print(\"start Iterative Policy Evaluation\")\n",
    "\n",
    "k = 1\n",
    "print()\n",
    "print(\"V0(S)   k = 0\")\n",
    "\n",
    "# ì´ˆê¸°í™”ëœ V í…Œì´ë¸” ì¶œë ¥\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ ë³€ìˆ˜ì— ì €ì¥\n",
    "start_time = time.time()\n",
    "\n",
    "# ë°˜ë³µ\n",
    "while(True):    \n",
    "    # 2. Î”â†0\n",
    "    delta = 0\n",
    "    # 3. vâ†(ğ‘ )\n",
    "    # ê³„ì‚°ì „ ê°€ì¹˜ë¥¼ ì €ì¥\n",
    "    temp_v = copy.deepcopy(v_table)\n",
    "    # 4. ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ : \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            G = 0\n",
    "            # 5. ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ìœ¼ë¡œ ë‹¤ìŒìƒíƒœë§Œ ì´ìš©í•´ ğ‘‰(ğ‘ ) ê³„ì‚°\n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                \n",
    "#                 print(\"s({0}): {1:5s} : {2:0.2f} = {3:0.2f} *({4:0.2f} +  {5:0.2f} *  {6:0.2f})\".format(i*env.reward.shape[0]+j,dic[action],agent.select_action_pr[action] * (reward + gamma*V[observation[0],observation[1]]), agent.select_action_pr[action],reward,gamma,V[observation[0],observation[1]]))\n",
    "\n",
    "                G += agent.select_action_pr[action] * (reward + gamma*v_table[observation[0],observation[1]])                    \n",
    "\n",
    "#             print(\"V{2}({0}) :sum = {1:.2f}\".format(i*env.reward.shape[0]+j,total,k))\n",
    "#             print()\n",
    "            v_table[i,j] = G\n",
    "    # 6. âˆ†â†maxâ¡(âˆ†,|vâˆ’ğ‘‰(ğ‘ )|)\n",
    "    # ê³„ì‚°ì „ê³¼ ê³„ì‚°í›„ì˜ ê°€ì¹˜ ì°¨ì´ ê³„ì‚°\n",
    "    delta = np.max([delta, np.max(np.abs(temp_v-v_table))])\n",
    "    \n",
    "    end_time = time.time()        \n",
    "    print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f} total_time = {3}\".format(k,k, delta,np.round(end_time-start_time),2))\n",
    "    show_v_table(np.round(v_table,2),env)                \n",
    "    k +=1\n",
    "\n",
    "    # 7. âˆ† <ğœƒê°€ ì‘ì€ ì–‘ìˆ˜ ì¼ ë•Œê¹Œì§€ ë°˜ë³µ\n",
    "\n",
    "    if delta < 0.000001:\n",
    "        break\n",
    "        \n",
    "end_time = time.time()        \n",
    "print(\"total_time = {}\".format(np.round(end_time-start_time),2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ë°˜ë³µ ì •ì±… í‰ê°€ (5by5, jemin)\n",
    "np.random.seed(0)\n",
    "env = Environment_jemin()\n",
    "agent = Agent_jemin()\n",
    "gamma = 0.99\n",
    "\n",
    "# 1. ëª¨ë“  ğ‘ âˆˆğ‘†^ì— ëŒ€í•´ì„œ ë°°ì—´ ğ‘‰(ğ‘ )=0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "\n",
    "print(\"start Iterative Policy Evaluation\")\n",
    "\n",
    "k = 1\n",
    "print()\n",
    "print(\"V0(S)   k = 0\")\n",
    "\n",
    "# ì´ˆê¸°í™”ëœ V í…Œì´ë¸” ì¶œë ¥\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ ë³€ìˆ˜ì— ì €ì¥\n",
    "start_time = time.time()\n",
    "\n",
    "# ë°˜ë³µ\n",
    "while(True):\n",
    "    # 2. Î”â†0\n",
    "    delta = 0\n",
    "    # 3. vâ†(ğ‘ )\n",
    "    # ê³„ì‚°ì „ ê°€ì¹˜ë¥¼ ì €ì¥\n",
    "    temp_v = copy.deepcopy(v_table)\n",
    "    # 4. ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ :\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            G = 0\n",
    "            # 5. ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ìœ¼ë¡œ ë‹¤ìŒìƒíƒœë§Œ ì´ìš©í•´ ğ‘‰(ğ‘ ) ê³„ì‚°\n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "\n",
    "#                 print(\"s({0}): {1:5s} : {2:0.2f} = {3:0.2f} *({4:0.2f} +  {5:0.2f} *  {6:0.2f})\".format(i*env.reward.shape[0]+j,dic[action],agent.select_action_pr[action] * (reward + gamma*V[observation[0],observation[1]]), agent.select_action_pr[action],reward,gamma,V[observation[0],observation[1]]))\n",
    "\n",
    "                G += agent.select_action_pr[action] * (reward + gamma*v_table[observation[0],observation[1]])\n",
    "\n",
    "#             print(\"V{2}({0}) :sum = {1:.2f}\".format(i*env.reward.shape[0]+j,total,k))\n",
    "#             print()\n",
    "            v_table[i,j] = G\n",
    "    # 6. âˆ†â†maxâ¡(âˆ†,|vâˆ’ğ‘‰(ğ‘ )|)\n",
    "    # ê³„ì‚°ì „ê³¼ ê³„ì‚°í›„ì˜ ê°€ì¹˜ ì°¨ì´ ê³„ì‚°\n",
    "    delta = np.max([delta, np.max(np.abs(temp_v-v_table))])\n",
    "\n",
    "    end_time = time.time()\n",
    "    #print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f} total_time = {3}\".format(k,k, delta,np.round(end_time-start_time),2))\n",
    "    #show_v_table_small(np.round(v_table,2),env)\n",
    "    k +=1\n",
    "\n",
    "    # 7. âˆ† <ğœƒê°€ ì‘ì€ ì–‘ìˆ˜ ì¼ ë•Œê¹Œì§€ ë°˜ë³µ\n",
    "\n",
    "    if delta < 0.00000001:\n",
    "        show_v_table_small(np.round(v_table,2),env)\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"total_time = {}\".format(np.round(end_time-start_time),2))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì •ì±… ë°˜ë³µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random V(S)\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.55      |       0.72      |       0.60      |       0.54      |       0.42      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.65      |       0.44      |       0.89      |       0.96      |       0.38      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.79      |       0.53      |       0.57      |       0.93      |       0.07      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.09      |       0.02      |       0.83      |       0.78      |       0.87      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.98      |       0.80      |       0.46      |       0.78      |       0.12      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "Initial random Policy Ï€0(S)\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†‘         |      â†’         |      â†’         |      â†’         |      â†         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†‘         |      â†         |      â†“         |      â†‘         |      â†         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†         |      â†“         |      â†         |      â†“         |      â†         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†‘         |      â†“         |      â†‘         |      â†‘         |      â†‘         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |      â†‘         |      â†‘         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "start policy iteration\n",
      "\n",
      "VÏ€0(S) delta = 0.0000009817\n",
      "\n",
      "policy Ï€1(S)\n",
      "\n",
      "VÏ€1(S) delta = 0.0000008809\n",
      "\n",
      "policy Ï€2(S)\n",
      "\n",
      "VÏ€2(S) delta = 0.0000000001\n",
      "\n",
      "policy Ï€3(S)\n",
      "\n",
      "VÏ€3(S) delta = 0.0000000001\n",
      "\n",
      "policy Ï€4(S)\n",
      "\n",
      "VÏ€4(S) delta = 0.0000000001\n",
      "\n",
      "policy Ï€5(S)\n",
      "\n",
      "VÏ€5(S) delta = 0.0000000000\n",
      "\n",
      "policy Ï€6(S)\n",
      "\n",
      "VÏ€6(S) delta = 0.0000000000\n",
      "\n",
      "policy Ï€7(S)\n",
      "+----------+----------+----------+----------+----------\n",
      "|   -0.43  |    0.63  |    1.81  |    0.63  |   -0.43  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    0.63  |    1.81  |    3.12  |    1.81  |    0.63  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    1.81  |    3.12  |    4.58  |    6.20  |    8.00  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    3.12  |    4.58  |    6.20  |    8.00  |   10.00  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    4.58  |    6.20  |    8.00  |   10.00  |   10.00  |\n",
      "+----------+----------+----------+----------+----------\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |      â†         |      â†         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†“         |      â†’         |      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†’         |      â†’         |      â†“         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†’         |      â†’         |      â†‘         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "total_time = 0.07819056510925293\n"
     ]
    }
   ],
   "source": [
    "# ìˆ™ì œ 1\n",
    "def policy_evalution(env, agent, v_table, policy):\n",
    "    gamma = 0.9\n",
    "    while(True):\n",
    "        # Î”â†0\n",
    "        delta = 0\n",
    "        #  vâ†ğ‘‰(ğ‘ )\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "        # ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ :\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                # ì—ì´ì „íŠ¸ë¥¼ ì§€ì •ëœ ì¢Œí‘œì— ìœ„ì¹˜ì‹œí‚¨í›„ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³„ì‚°\n",
    "                agent.set_pos([i,j])\n",
    "                # í˜„ì¬ ì •ì±…ì˜ í–‰ë™ì„ ì„ íƒ\n",
    "                action = policy[i,j]\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                v_table[i,j] = reward + gamma * v_table[observation[0],observation[1]]\n",
    "        # âˆ†â†maxâ¡(âˆ†,|vâˆ’ğ‘‰(ğ‘ )|)\n",
    "        # ê³„ì‚°ì „ê³¼ ê³„ì‚°í›„ì˜ ê°€ì¹˜ì˜ ì°¨ì´ë¥¼ ê³„ì‚°\n",
    "        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n",
    "                \n",
    "        # 7. âˆ† <ğœƒê°€ ì‘ì€ ì–‘ìˆ˜ ì¼ ë•Œê¹Œì§€ ë°˜ë³µ\n",
    "        if delta < 0.000001:\n",
    "            break\n",
    "    return v_table, delta\n",
    "\n",
    "\n",
    "def policy_improvement(env, agent, v_table, policy):\n",
    "    \n",
    "    # 67í˜ì´ì§€ ì•„ë˜ ëˆ„ë½ ë˜ì–´ìˆìŠµë‹ˆë‹¤\n",
    "    gamma = 0.9  \n",
    "    \n",
    "    # policyStable â† true \n",
    "    policyStable = True\n",
    "\n",
    "    # ëª¨ë“  sâˆˆSì— ëŒ€í•´ï¼š\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):            \n",
    "            # ğ‘œğ‘™ğ‘‘âˆ’ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†Ï€(s) \n",
    "            old_action = policy[i,j]            \n",
    "            # ê°€ëŠ¥í•œ í–‰ë™ì¤‘ ìµœëŒ“ê°’ì„ ê°€ì§€ëŠ” í–‰ë™ì„ ì„ íƒ\n",
    "            temp_action = 0\n",
    "            temp_value =  -1e+10           \n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent,action)\n",
    "                if temp_value < reward + gamma * v_table[observation[0],observation[1]]:\n",
    "                    temp_action = action\n",
    "                    temp_value = reward + gamma * v_table[observation[0],observation[1]]\n",
    "            # ë§Œì•½ ğ‘œğ‘™ğ‘‘âˆ’ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›\"â‰ Ï€(s)\"ë¼ë©´ï¼Œ \"policyStable â† False\" \n",
    "            # old-actionê³¼ ìƒˆë¡œìš´ actionì´ ë‹¤ë¥¸ì§€ ì²´í¬\n",
    "            if old_action != temp_action :\n",
    "                policyStable = False\n",
    "            policy[i,j] = temp_action\n",
    "    return policy, policyStable\n",
    "\n",
    "# ì •ì±… ë°˜ë³µ\n",
    "# í™˜ê²½ê³¼ ì—ì´ì „íŠ¸ì— ëŒ€í•œ ì´ˆê¸° ì„¤ì •\n",
    "np.random.seed(0)\n",
    "env = Environment_jemin()\n",
    "agent = Agent_jemin()\n",
    "\n",
    "# 1. ì´ˆê¸°í™”\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ ğ‘‰(ğ‘ )âˆˆğ‘…ê³¼ Ï€(ğ‘ )âˆˆğ´(ğ‘ )ë¥¼ ì„ì˜ë¡œ ì„¤ì •\n",
    "v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "policy = np.random.randint(0, 4,(env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "print(\"Initial random V(S)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()\n",
    "print(\"Initial random Policy Ï€0(S)\")\n",
    "show_policy(policy,env)\n",
    "print(\"start policy iteration\")\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ì„ ë³€ìˆ˜ì— ì €ì¥\n",
    "start_time = time.time()\n",
    "\n",
    "max_iter_number = 20000\n",
    "for iter_number in range(max_iter_number):\n",
    "    \n",
    "    # 2.ì •ì±…í‰ê°€\n",
    "    v_table, delta = policy_evalution(env, agent, v_table, policy)\n",
    "\n",
    "    # ì •ì±… í‰ê°€ í›„ ê²°ê³¼ í‘œì‹œ                                            \n",
    "    print(\"\")\n",
    "    print(\"VÏ€{0:}(S) delta = {1:.10f}\".format(iter_number,delta))\n",
    "    #show_v_table_small(np.round(v_table,2),env)\n",
    "    print()    \n",
    "    \n",
    "    \n",
    "    # 3.ì •ì±…ê°œì„ \n",
    "    policy, policyStable = policy_improvement(env, agent, v_table, policy)\n",
    "\n",
    "    # policy ë³€í™” ì €ì¥\n",
    "    print(\"policy Ï€{}(S)\".format(iter_number+1))\n",
    "    #show_policy(policy,env)\n",
    "    # í•˜ë‚˜ë¼ë„ old-actionê³¼ ìƒˆë¡œìš´ actionì´ ë‹¤ë¥´ë‹¤ë©´ '2. ì •ì±…í‰ê°€'ë¥¼ ë°˜ë³µ\n",
    "    if(policyStable == True):\n",
    "        show_v_table_small(np.round(v_table,2),env)\n",
    "        show_policy(policy,env)\n",
    "        break\n",
    "\n",
    "print(\"total_time = {}\".format(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°€ì¹˜ë°˜ë³µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random V0(S)\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.55      |       0.72      |       0.60      |       0.54      |       0.42      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.65      |       0.44      |       0.89      |       0.96      |       0.38      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.79      |       0.53      |       0.57      |       0.93      |       0.07      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.09      |       0.02      |       0.83      |       0.78      |       0.87      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.98      |       0.80      |       0.46      |       0.78      |       0.12      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "start Value iteration\n",
      "\n",
      "V154(S) : k = 154    delta = 0.000000\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      -0.43      |       0.63      |       1.81      |       0.63      |      -0.43      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.63      |       1.81      |       3.12      |       1.81      |       0.63      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       1.81      |       3.12      |       4.58      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       3.12      |       4.58      |       6.20      |       8.00      |      10.00      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       4.58      |       6.20      |       8.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "total_time = 0.0\n",
      "\n",
      "Optimal policy\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |      â†         |      â†         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†“         |      â†’         |      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†’         |      â†’         |      â†“         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†’         |      â†’         |      â†‘         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "#ìˆ™ì œ 2\n",
    "def finding_optimal_value_function(env, agent, v_table):\n",
    "    k = 1\n",
    "    gamma = 0.9\n",
    "    while(True):\n",
    "        # Î”â†0\n",
    "        delta=0\n",
    "        #  vâ†ğ‘‰(ğ‘ )\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "\n",
    "        # ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ :\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                temp = -1e+10\n",
    "#                 print(\"s({0}):\".format(i*env.reward.shape[0]+j))\n",
    "                # ğ‘‰(ğ‘ )â† max(a)â¡âˆ‘ğ‘ƒ(ğ‘ '|ğ‘ ,ğ‘)[ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ ') +ğ›¾ğ‘‰(ğ‘ ')]\n",
    "                # ê°€ëŠ¥í•œ í–‰ë™ì„ ì„ íƒ\n",
    "                for action in range(len(agent.action)):\n",
    "                    agent.set_pos([i,j])\n",
    "                    observation, reward, done = env.move(agent, action)\n",
    "#                     print(\"{0:.2f} = {1:.2f} + {2:.2f} * {3:.2f}\" .format(reward + gamma* v_table[observation[0],observation[1]],reward, gamma,v_table[observation[0],observation[1]]))\n",
    "                    #ì´ë™í•œ ìƒíƒœì˜ ê°€ì¹˜ê°€ tempë³´ë‹¤ í¬ë©´\n",
    "                    if temp < reward + gamma*v_table[observation[0],observation[1]]:\n",
    "                        # temp ì— ìƒˆë¡œìš´ ê°€ì¹˜ë¥¼ ì €ì¥\n",
    "                        temp = reward + gamma*v_table[observation[0],observation[1]]  \n",
    "#                 print(\"V({0}) :max = {1:.2f}\".format(i*env.reward.shape[0]+j,temp))\n",
    "#                 print()\n",
    "                # ì´ë™ ê°€ëŠ¥í•œ ìƒíƒœ ì¤‘ ê°€ì¥ í° ê°€ì¹˜ë¥¼ ì €ì¥\n",
    "                v_table[i,j] = temp\n",
    "\n",
    "        #  âˆ†â†maxâ¡(âˆ†,|vâˆ’ğ‘‰(ğ‘ )|)\n",
    "        # ì´ì „ ê°€ì¹˜ì™€ ë¹„êµí•´ì„œ í° ê°’ì„ deltaì— ì €ì¥\n",
    "        # ê³„ì‚°ì „ê³¼ ê³„ì‚°í›„ì˜ ê°€ì¹˜ì˜ ì°¨ì´ ê³„ì‚°\n",
    "        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n",
    "        # 7. âˆ† <ğœƒê°€ ì‘ì€ ì–‘ìˆ˜ ì¼ ë•Œê¹Œì§€ ë°˜ë³µ\n",
    "        if delta < 0.0000001:\n",
    "            print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n",
    "            show_v_table(np.round(v_table,2),env)\n",
    "            break\n",
    "            \n",
    "#         if k < 4 or k > 150:\n",
    "#             print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n",
    "#             show_v_table(np.round(v_table,2),env)\n",
    "        #print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n",
    "        #show_v_table(np.round(v_table,2),env)\n",
    "        k +=1\n",
    "        \n",
    "    return v_table\n",
    "\n",
    "def policy_extraction(env, agent, v_table, optimal_policy):\n",
    "\n",
    "    gamma = 0.9\n",
    "    \n",
    "    #ì •ì±… ğœ‹ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ì¶œ\n",
    "    # ğœ‹(ğ‘ )â† argmax(a)â¡âˆ‘ğ‘ƒ(ğ‘ '|ğ‘ ,ğ‘)[ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘ ') +ğ›¾ğ‘‰(ğ‘ ')]\n",
    "    # ëª¨ë“  ğ‘ âˆˆğ‘†ì— ëŒ€í•´ : \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            temp =  -1e+10\n",
    "            # ê°€ëŠ¥í•œ í–‰ë™ì¤‘ ê°€ì¹˜ê°€ ê°€ì¥ë†’ì€ ê°’ì„ policy[i,j]ì— ì €ì¥\n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent,action)\n",
    "                if temp < reward + gamma * v_table[observation[0],observation[1]]:\n",
    "                    optimal_policy[i,j] = action\n",
    "                    temp = reward + gamma * v_table[observation[0],observation[1]]\n",
    "                \n",
    "    return optimal_policy\n",
    "\n",
    "\n",
    "# ê°€ì¹˜ ë°˜ë³µ\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "np.random.seed(0)\n",
    "env = Environment_jemin()\n",
    "agent = Agent_jemin()\n",
    "\n",
    "# ì´ˆê¸°í™”\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†^+ì— ëŒ€í•´ ğ‘‰(ğ‘ )âˆˆğ‘…ì„ ì„ì˜ë¡œ ì„¤ì •\n",
    "v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "\n",
    "print(\"Initial random V0(S)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()\n",
    "\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "print(\"start Value iteration\")\n",
    "print()\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ ë³€ìˆ˜ì— ì €ì¥\n",
    "start_time = time.time()\n",
    "\n",
    "v_table = finding_optimal_value_function(env, agent, v_table)\n",
    "\n",
    "optimal_policy = policy_extraction(env, agent, v_table, optimal_policy)\n",
    "\n",
    "                \n",
    "print(\"total_time = {}\".format(np.round(time.time()-start_time),2))\n",
    "print()\n",
    "print(\"Optimal policy\")\n",
    "show_policy(optimal_policy, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—í”¼ì†Œë“œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent, first_visit):\n",
    "    gamma = 0.09\n",
    "    # ì—í”¼ì†Œë“œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    episode = []\n",
    "    # ì´ì „ì— ë°©ë¬¸ì—¬ë¶€ ì²´í¬\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    \n",
    "    # ì—ì´ì „íŠ¸ê°€ ëª¨ë“  ìƒíƒœì—ì„œ ì¶œë°œí•  ìˆ˜ ìˆê²Œ ì¶œë°œì§€ì ì„ ë¬´ì‘ìœ„ë¡œ ì„¤ì •\n",
    "    i = np.random.randint(0,env.reward.shape[0])\n",
    "    j = np.random.randint(0,env.reward.shape[1])\n",
    "    agent.set_pos([i,j])    \n",
    "    #ì—í”¼ì†Œë“œì˜ ìˆ˜ìµì„ ì´ˆê¸°í™”\n",
    "    G = 0\n",
    "    #ê°ì‡„ìœ¨ì˜ ì§€ìˆ˜\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    # ì—í”¼ì†Œë“œ ìƒì„±\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()            \n",
    "        action = np.random.randint(0,len(agent.action))            \n",
    "        observaetion, reward, done = env.move(agent, action)    \n",
    "        \n",
    "        if first_visit:\n",
    "            # ì—í”¼ì†Œë“œì— ì²« ë°©ë¬¸í•œ ìƒíƒœì¸ì§€ ê²€ì‚¬ :\n",
    "            # visit[pos[0],pos[1]] == 0 : ì²« ë°©ë¬¸\n",
    "            # visit[pos[0],pos[1]] == 1 : ì¤‘ë³µ ë°©ë¬¸\n",
    "            if visit[pos[0],pos[1]] == 0:   \n",
    "                # ì—í”¼ì†Œë“œê°€ ëë‚ ë•Œê¹Œì§€ Gë¥¼ ê³„ì‚°\n",
    "                G += gamma**(step) * reward        \n",
    "                # ë°©ë¬¸ ì´ë ¥ í‘œì‹œ\n",
    "                visit[pos[0],pos[1]] = 1\n",
    "                step += 1               \n",
    "                # ë°©ë¬¸ ì´ë ¥ ì €ì¥(ìƒíƒœ, í–‰ë™, ë³´ìƒ)\n",
    "                episode.append((pos,action, reward))\n",
    "        else:\n",
    "            G += gamma**(step) * reward\n",
    "            step += 1                   \n",
    "            episode.append((pos,action,reward))            \n",
    "\n",
    "        # ì—í”¼ì†Œë“œê°€ ì¢…ë£Œí–ˆë‹¤ë©´ ë£¨í”„ì—ì„œ íƒˆì¶œ\n",
    "        if done == True:                \n",
    "            break        \n",
    "            \n",
    "    return i, j, G, episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-visit and Every-Visit MC Prediction\n",
    "- í•´ë‹¹ êµ¬í˜„ì€ êµì œì™€ ë‹¤ë¥´ê²Œ ìŠ¤íƒ€íŒ… í¬ì¸íŠ¸ê°€ ë‹¤ë¥¸ ê²½ìš°ì— ëŒ€í•´ì„œë§Œ ê°±ì‹ í•œë‹¤.\n",
    "- ì›ë˜ ì˜ì‚¬ì½”ë“œëŠ” ì´ì „ì— ë°œìƒí•œ ê²ƒìœ¼ë¡œ ê°±ì‹  í•´ì•¼í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first-visit MC and every-visit MC prediction\n",
    "np.random.seed(0)\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# ì„ì˜ì˜ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ğ‘‰\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ìƒíƒœë³„ë¡œ ì—í”¼ì†Œë“œ ì¶œë°œíšŸìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "v_start = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ìƒíƒœë³„ë¡œ ë„ì°©ì§€ì  ë„ì°©íšŸìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "v_success = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›(ğ‘ )â†ë¹ˆ ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  sâˆˆğ‘†ì— ëŒ€í•´)\n",
    "Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ë¥¼ ì§€ì •\n",
    "max_episode = 100000\n",
    "\n",
    "# first visit ë¥¼ ì‚¬ìš©í• ì§€ every visitë¥¼ ì‚¬ìš©í•  ì§€ ê²°ì •\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    \n",
    "    i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "    \n",
    "    # ìˆ˜ìµ ğºë¥¼ ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›(ğ‘ )ì— ì¶”ê°€(append)\n",
    "    Return_s[i][j].append(G)\n",
    "    \n",
    "    # ì—í”¼ì†Œë“œ ë°œìƒ íšŸìˆ˜ ê³„ì‚°\n",
    "    episode_count = len(Return_s[i][j])\n",
    "    # ìƒíƒœë³„ ë°œìƒí•œ ìˆ˜ìµì˜ ì´í•© ê³„ì‚°\n",
    "    total_G = np.sum(Return_s[i][j])\n",
    "    # ìƒíƒœë³„ ë°œìƒí•œ ìˆ˜ìµì˜ í‰ê·  ê³„ì‚°\n",
    "    v_table[i,j] = total_G / episode_count\n",
    "    \n",
    "  # ë„ì°©ì§€ì ì— ë„ì°©(reward = 1)í–ˆëŠ”ì§€ ì²´í¬    \n",
    "    # episode[-1][2] : ì—í”¼ì†Œë“œ ë§ˆì§€ë§‰ ìƒíƒœì˜ ë³´ìƒ\n",
    "    if episode[-1][2] == 1:\n",
    "        v_success[i,j] += 1\n",
    "\n",
    "# ì—í”¼ì†Œë“œ ì¶œë°œ íšŸìˆ˜ ì €ì¥ \n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        v_start[i,j] = len(Return_s[i][j])\n",
    "        \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print(\"V_start_count(s)\")\n",
    "show_v_table(np.round(v_start,2),env)\n",
    "print(\"V_success_pr(s)\")\n",
    "show_v_table(np.round(v_success/v_start,2),env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental mean ì„ ì´ìš©í•˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ Prediction ì•Œê³ ë¦¬ì¦˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incremental mean ì„ ì´ìš©í•˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ Prediction ì•Œê³ ë¦¬ì¦˜\n",
    "np.random.seed(0)\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# ì„ì˜ì˜ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ğ‘‰\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ì¶”ê°€\n",
    "# ìƒíƒœë¥¼ ë°©ë¬¸í•œ íšŸìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "v_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ì‚­ì œ\n",
    "# # ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›(ğ‘ )â†ë¹ˆ ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  sâˆˆğ‘†ì— ëŒ€í•´) : \n",
    "# Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ì™€ ì—í”¼ì†Œë“œ ìµœëŒ€ ê¸¸ì´ì§€ì •\n",
    "max_episode = 100000\n",
    "\n",
    "# first visitì„ ì‚¬ìš©í• ì§€ every visitì„ ì‚¬ìš©í•  ì§€ ê²°ì •\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    \n",
    "    i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "    \n",
    "    # ì‚­ì œ\n",
    "    # ìˆ˜ìµ ğºë¥¼ ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›(ğ‘ )ì— ì¶”ê°€(append)\n",
    "    #Return_s[i][j].append(G)\n",
    "    \n",
    "    # ì—í”¼ì†Œë“œ ë°œìƒ íšŸìˆ˜ ê³„ì‚°\n",
    "    #episode_count = len(Return_s[i][j])\n",
    "    # ìƒíƒœë³„ ë°œìƒí•œ ìˆ˜ìµì˜ ì´í•© ê³„ì‚°\n",
    "    #total_G = np.sum(Return_s[i][j])\n",
    "    # ìƒíƒœë³„ ë°œìƒí•œ ìˆ˜ìµì˜ í‰ê·  ê³„ì‚°\n",
    "    #v_table[i,j] = total_G / episode_count\n",
    "    \n",
    "    #Return_length[i][j].append(len(episode))\n",
    "\n",
    "    #if episode[-1][2] == 1:\n",
    "    #    v_success[i,j] += 1    \n",
    "    \n",
    "    # Incremental mean  í‰ê· ì„ ê³„ì‚°\n",
    "    v_visit[i,j] += 1\n",
    "    v_table[i,j] += 1 / v_visit[i,j] * (G - v_table[i,j])\n",
    "    \n",
    "      \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ì—í”¼ì†Œë“œ ë¶„ë¦¬ë¥¼ ì´ìš©í•˜ëŠ” Firsit-visit ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²•ê³¼ Every-visit ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²•ì˜ Prediction ì•Œê³ ë¦¬ì¦˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent, first_visit):\n",
    "    gamma = 0.09\n",
    "    # ì—í”¼ì†Œë“œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    episode = []\n",
    "    # ì´ì „ì— ë°©ë¬¸ì—¬ë¶€ ì²´í¬\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    \n",
    "    # ì—ì´ì „íŠ¸ëŠ” í•­ìƒ (0,0)ì—ì„œ ì¶œë°œ\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])    \n",
    "    #ì—í”¼ì†Œë“œì˜ ìˆ˜ìµì„ ì´ˆê¸°í™”\n",
    "    G = 0\n",
    "    #ê°ì‡„ìœ¨ì˜ ì§€ìˆ˜\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    # ì—í”¼ì†Œë“œ ìƒì„±\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()            \n",
    "        action = np.random.randint(0,len(agent.action))            \n",
    "        observaetion, reward, done = env.move(agent, action)    \n",
    "        \n",
    "        if first_visit:\n",
    "            # ì—í”¼ì†Œë“œì— ì²« ë°©ë¬¸í•œ ìƒíƒœì¸ì§€ ê²€ì‚¬ :\n",
    "            # visit[pos[0],pos[1]] == 0 : ì²« ë°©ë¬¸\n",
    "            # visit[pos[0],pos[1]] == 1 : ì¤‘ë³µ ë°©ë¬¸\n",
    "            if visit[pos[0],pos[1]] == 0:   \n",
    "                # ì—í”¼ì†Œë“œê°€ ëë‚ ë•Œê¹Œì§€ Gë¥¼ ê³„ì‚°\n",
    "                G += gamma**(step) * reward        \n",
    "                # ë°©ë¬¸ ì´ë ¥ í‘œì‹œ\n",
    "                visit[pos[0],pos[1]] = 1\n",
    "                step += 1               \n",
    "                # ë°©ë¬¸ ì´ë ¥ ì €ì¥(ìƒíƒœ, í–‰ë™, ë³´ìƒ)\n",
    "                episode.append((pos,action, reward))\n",
    "        else:\n",
    "            G += gamma**(step) * reward\n",
    "            step += 1                   \n",
    "            episode.append((pos,action, reward))            \n",
    "\n",
    "        # ì—í”¼ì†Œë“œê°€ ì¢…ë£Œí–ˆë‹¤ë©´ ë£¨í”„ì—ì„œ íƒˆì¶œ\n",
    "        if done == True:                \n",
    "            break        \n",
    "            \n",
    "    return i, j, G, episode\n",
    "\n",
    "#ì—í”¼ì†Œë“œ ë¶„ë¦¬ë¥¼ ì´ìš©í•˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ Prediction ì•Œê³ ë¦¬ì¦˜\n",
    "np.random.seed(0)\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# ì„ì˜ì˜ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ğ‘‰\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ìƒíƒœë¥¼ ë°©ë¬¸í•œ íšŸìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "v_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ì™€ ì—í”¼ì†Œë“œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •\n",
    "max_episode = 99999\n",
    "\n",
    "# first visit ë¥¼ ì‚¬ìš©í• ì§€ every visitë¥¼ ì‚¬ìš©í•  ì§€ ê²°ì •\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "gamma = 0.09\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    \n",
    "    i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "    for step_num in range(len(episode)):\n",
    "        G = 0\n",
    "        # episode[step_num][0][0] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ x ì¢Œí‘œ\n",
    "        # episode[step_num][0][1] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ y ì¢Œí‘œ\n",
    "        # episode[step_num][1] : step_numë²ˆì§¸ ìƒíƒœì—ì„œ ì„ íƒí•œ í–‰ë™\n",
    "        i = episode[step_num][0][0]\n",
    "        j = episode[step_num][0][1]\n",
    "        # ì—í”¼ì†Œë“œ ì‹œì‘ì ì„ ì¹´ìš´íŠ¸\n",
    "        v_visit[i,j] += 1\n",
    "\n",
    "        # ì„œë¸Œ ì—í”¼ì†Œë“œ (episode[step_num:])ì˜ ì¶œë°œë¶€í„° ëê¹Œì§€ ë³´ìˆ˜ Gë¥¼ ê³„ì‚°\n",
    "        # k[2] : episode[step_num][2] ê³¼ ê°™ìœ¼ë©° step_num ë²ˆì§¸ ë°›ì€ ë³´ìƒ\n",
    "        # step : ê°ì‡„ìœ¨\n",
    "        for step, k in enumerate(episode[step_num:]):\n",
    "            G += gamma**(step)*k[2]\n",
    "            \n",
    "        # Incremental mean  í‰ê· ì„ ê³„ì‚°\n",
    "        v_table[i,j] += 1 / v_visit[i,j] * (G - v_table[i,j])\n",
    "           \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print(\"V_start_count(s)\")\n",
    "show_v_table(np.round(v_visit,2),env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e-greedy í™•ë¥  ë³€í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episilon |   a1  |    a2   |    a3  |   a4  \n",
      "1.0      | 0.250 |  0.250  |  0.250 |  0.250\n",
      "0.9      | 0.225 |  0.225  |  0.225 |  0.325\n",
      "0.8      | 0.200 |  0.200  |  0.200 |  0.400\n",
      "0.7      | 0.175 |  0.175  |  0.175 |  0.475\n",
      "0.6      | 0.150 |  0.150  |  0.150 |  0.550\n",
      "0.5      | 0.125 |  0.125  |  0.125 |  0.625\n",
      "0.4      | 0.100 |  0.100  |  0.100 |  0.700\n",
      "0.3      | 0.075 |  0.075  |  0.075 |  0.775\n",
      "0.2      | 0.050 |  0.050  |  0.050 |  0.850\n",
      "0.1      | 0.025 |  0.025  |  0.025 |  0.925\n",
      "0.0      | 0.000 |  0.000  |  0.000 |  1.000\n"
     ]
    }
   ],
   "source": [
    "policy = np.array((0.1,0.2,0.3,0.4))\n",
    "up_list=[]\n",
    "right_list=[]\n",
    "down_list=[]\n",
    "left_list=[]\n",
    "print(\"episilon |   a1  |    a2   |    a3  |   a4  \")\n",
    "for epsilon in reversed(range(0, 11)):\n",
    "    epsilon /= 10\n",
    "    up = epsilon / len(policy)\n",
    "    right = epsilon / len(policy)\n",
    "    down = epsilon / len(policy)\n",
    "    left = 1 -  epsilon + epsilon / len(policy)\n",
    "    print(\"{0:}      | {1:.3f} |  {2:.3f}  |  {3:.3f} |  {4:.3f}\".format(np.round(epsilon,3),np.round(up,3),np.round(right,3),np.round(down,3),np.round(left,3)))\n",
    "#     policy_list.append((up,right,down,left))\n",
    "#     up_list.append(up)\n",
    "#     right_list.append(right)\n",
    "#     down_list.append(down)\n",
    "#     left_list.append(left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ïµ-ì •ì±…ì„ ì´ìš©í•˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ control ì•Œê³ ë¦¬ì¦˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_episode_with_policy(env, agent, first_visit, policy):\n",
    "    gamma = 0.09\n",
    "    # ì—í”¼ì†Œë“œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    episode = []\n",
    "    # ì´ì „ì— ë°©ë¬¸ì—¬ë¶€ ì²´í¬\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    \n",
    "    # ì—ì´ì „íŠ¸ëŠ” í•­ìƒ (0,0)ì—ì„œ ì¶œë°œ\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])    \n",
    "    #ì—í”¼ì†Œë“œì˜ ìˆ˜ìµì„ ì´ˆê¸°í™”\n",
    "    G = 0\n",
    "    #ê°ì‡„ìœ¨ì˜ ì§€ìˆ˜\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    # ì—í”¼ì†Œë“œ ìƒì„±\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()        \n",
    "        # í˜„ì¬ ìƒíƒœì˜ ì •ì±…ì„ ì´ìš©í•´ í–‰ë™ì„ ì„ íƒí•œ í›„ ì´ë™\n",
    "        action = np.random.choice(range(0,len(agent.action)), p=policy[pos[0],pos[1],:]) \n",
    "        observaetion, reward, done = env.move(agent, action)    \n",
    "        \n",
    "        if first_visit:\n",
    "            # ì—í”¼ì†Œë“œì— ì²« ë°©ë¬¸í•œ ìƒíƒœì¸ì§€ ê²€ì‚¬ :\n",
    "            # visit[pos[0],pos[1]] == 0 : ì²« ë°©ë¬¸\n",
    "            # visit[pos[0],pos[1]] == 1 : ì¤‘ë³µ ë°©ë¬¸\n",
    "            if visit[pos[0],pos[1],action] == 0:   \n",
    "                # ì—í”¼ì†Œë“œê°€ ëë‚ ë•Œê¹Œì§€ Gë¥¼ ê³„ì‚°\n",
    "                G += gamma**(step) * reward        \n",
    "                # ë°©ë¬¸ ì´ë ¥ í‘œì‹œ\n",
    "                visit[pos[0],pos[1],action] = 1\n",
    "                step += 1               \n",
    "                # ë°©ë¬¸ ì´ë ¥ ì €ì¥(ìƒíƒœ, í–‰ë™, ë³´ìƒ)\n",
    "                episode.append((pos,action, reward))\n",
    "        else:\n",
    "            G += gamma**(step) * reward\n",
    "            step += 1                   \n",
    "            episode.append((pos,action,reward))            \n",
    "\n",
    "        # ì—í”¼ì†Œë“œê°€ ì¢…ë£Œí–ˆë‹¤ë©´ ë£¨í”„ì—ì„œ íƒˆì¶œ\n",
    "        if done == True:                \n",
    "            break        \n",
    "            \n",
    "    return i, j, G, episode\n",
    "\n",
    "# Ïµ-ì •ì±…ì„ ì´ìš©í•˜ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ control ì•Œê³ ë¦¬ì¦˜\n",
    "np.random.seed(0)\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# # ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’ (í–‰ë™ ê°œìˆ˜, ë¯¸ë¡œ ì„¸ë¡œ, ë¯¸ë¡œ ê°€ë¡œ)\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "print(\"Initial Q(s,a)\")\n",
    "show_q_table(Q_table,env)\n",
    "\n",
    "# ìƒíƒœë¥¼ ë°©ë¬¸í•œ íšŸìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "Q_visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "# ë¯¸ë¡œ ëª¨ë“  ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ì €ì¥í•˜ëŠ” í…Œì´ë¸”\n",
    "# ê° ìƒíƒœì—ì„œ Q ê°’ì´ ê°€ì¥ í° í–‰ë™ì„ ì„ íƒ í›„ optimal_a ì— ì €ì¥\n",
    "optimal_a = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_a[i,j] = np.argmax(Q_table[i,j,:])\n",
    "print(\"initial optimal_a\")\n",
    "show_policy(optimal_a,env)\n",
    "\n",
    "# Ï€(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ğœ–âˆ’íƒìš• ì •ì±…\n",
    "# ë¬´ì‘ìœ„ë¡œ í–‰ë™ì„ ì„ íƒí•˜ë„ë¡ ì§€ì •\n",
    "policy = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "# í•œ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ê³„ì‚°\n",
    "epsilon = 0.8\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        for k in range(len(agent.action)):\n",
    "            if optimal_a[i,j] == k:\n",
    "                policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n",
    "            else:\n",
    "                policy[i,j,k] = epsilon/len(agent.action)\n",
    "print(\"Initial Policy\")\n",
    "show_q_table(policy,env)\n",
    "\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ ìˆ˜ ê¸¸ì´ë¥¼ ì§€ì •\n",
    "max_episode = 10000\n",
    "\n",
    "# first visit ë¥¼ ì‚¬ìš©í• ì§€ every visitë¥¼ ì‚¬ìš©í•  ì§€ ê²°ì •\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "gamma = 0.09\n",
    "for epi in tqdm(range(max_episode)):\n",
    "# for epi in range(max_episode):\n",
    "\n",
    "    # Ï€ë¥¼ ì´ìš©í•´ì„œ ì—í”¼ì†Œë“œ 1ê°œë¥¼ ìƒì„±\n",
    "    x,y,G,episode = generate_episode_with_policy(env, agent, first_visit, policy)\n",
    "    \n",
    "    for step_num in range(len(episode)):\n",
    "        G = 0\n",
    "        # episode[step_num][0][0] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ x ì¢Œí‘œ\n",
    "        # episode[step_num][0][1] : step_numë²ˆì§¸ ë°©ë¬¸í•œ ìƒíƒœì˜ y ì¢Œí‘œ\n",
    "        # episode[step_num][1] : step_numë²ˆì§¸ ìƒíƒœì—ì„œ ì„ íƒí•œ í–‰ë™\n",
    "        i = episode[step_num][0][0]\n",
    "        j = episode[step_num][0][1]\n",
    "        action = episode[step_num][1]\n",
    "        \n",
    "        # ì—í”¼ì†Œë“œ ì‹œì‘ì ì„ ì¹´ìš´íŠ¸\n",
    "        Q_visit[i,j,action] += 1\n",
    "\n",
    "        # ì„œë¸Œ ì—í”¼ì†Œë“œ (episode[step_num:])ì˜ ì¶œë°œë¶€í„° ëê¹Œì§€ ìˆ˜ìµ Gë¥¼ ê³„ì‚°\n",
    "        # k[2] : episode[step_num][2] ê³¼ ê°™ìœ¼ë©° step_num ë²ˆì§¸ ë°›ì€ ë³´ìƒ\n",
    "        # step : ê°ì‡„ìœ¨\n",
    "        for step, k in enumerate(episode[step_num:]):\n",
    "            G += gamma**(step)*k[2]\n",
    "\n",
    "        # Incremental mean : ğ‘„(ğ‘ ,ğ‘)â†ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’(ğ‘…ğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›(ğ‘ ,ğ‘)) \n",
    "        Q_table[i,j,action] += 1 / Q_visit[i,j,action]*(G-Q_table[i,j,action])\n",
    "    \n",
    "    # (c) ì—í”¼ì†Œë“œ ì•ˆì˜ ê° sì— ëŒ€í•´ì„œ :\n",
    "    # ë¯¸ë¡œ ëª¨ë“  ìƒíƒœì—ì„œ ìµœì  í–‰ë™ì„ ì €ì¥í•  ê³µê°„ ë§ˆë ¨\n",
    "    # ğ‘âˆ— â†argmax_a ğ‘„(ğ‘ ,ğ‘)\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            optimal_a[i,j] = np.argmax(Q_table[i,j,:])            \n",
    "   \n",
    "    # ëª¨ë“  ğ‘âˆˆğ´(ğ‘†) ì— ëŒ€í•´ì„œ :\n",
    "    # ìƒˆë¡œ ê³„ì‚°ëœ optimal_a ë¥¼ ì´ìš©í•´ì„œ í–‰ë™ ì„ íƒ í™•ë¥  policy (Ï€) ê°±ì‹ \n",
    "    epsilon = 1 - epi/max_episode\n",
    "\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            for k in range(len(agent.action)):\n",
    "                if optimal_a[i,j] == k:\n",
    "                    policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n",
    "                else:\n",
    "                    policy[i,j,k] = epsilon/len(agent.action)\n",
    "\n",
    "print(\"Final Q(s,a)\")\n",
    "show_q_table(Q_table,env)\n",
    "print(\"Final policy\")\n",
    "show_q_table(policy,env)\n",
    "print(\"Final optimal_a\")\n",
    "show_policy(optimal_a,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD(0) prediction\n",
    "np.random.seed(0)\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#ì´ˆê¸°í™” : \n",
    "#Ï€â† í‰ê°€í•  ì •ì±…\n",
    "# ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ë„ë¡ ì§€ì •\n",
    "#ğ‘‰â† ì„ì˜ì˜ ìƒíƒœê°€ì¹˜ í•¨ìˆ˜\n",
    "V = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ, ì—í”¼ì†Œë“œì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "print(\"start TD(0) prediction\")\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta =0\n",
    "    # s ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    #  ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # aâ†ìƒíƒœ ğ‘  ì—ì„œ ì •ì±… Ï€ì— ì˜í•´ ê²°ì •ëœ í–‰ë™ \n",
    "        # ê°€ëŠ¥í•œ ëª¨ë“  í–‰ë™ì´ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ê²Œ í•¨\n",
    "        action = np.random.randint(0,4)\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìˆ˜ rê³¼ ë‹¤ìŒ ìƒíƒœ sâ€™ë¥¼ ê´€ì¸¡\n",
    "        # sâ†ğ‘ '\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        # V(ğ‘ )â†V(ğ‘ )+ Î±[ğ‘Ÿ+ğ›¾ğ‘‰(ğ‘ ^)âˆ’ğ‘‰(ğ‘ )]\n",
    "        V[pos[0],pos[1]] += alpha * (reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]])\n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(V,2),env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦, ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦  í•¨ìˆ˜ ì‘ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦, ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦  í•¨ìˆ˜ ì‘ì„±\n",
    "def e_greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    greedy_action = np.argmax(Q_table[pos[0],pos[1],:])\n",
    "    pr = np.zeros(4)\n",
    "    for i in range(len(agent.action)):\n",
    "        if i == greedy_action:\n",
    "            pr[i] = 1 - epsilon + epsilon/len(agent.action)\n",
    "        else:\n",
    "            pr[i] = epsilon / len(agent.action)\n",
    "\n",
    "    return np.random.choice(range(0,len(agent.action)), p=pr)    \n",
    "\n",
    "def greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    return np.argmax(Q_table[pos[0],pos[1],:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) coltrol : SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 965/10000 [00:00<00:01, 4719.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) control : SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:01<00:00, 5115.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA : Q(s,a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -9.20       |     -7.30       |     -5.16       |\n",
      "| -9.71     -5.47 | -7.55     -4.38 | -5.91     -6.55 |\n",
      "|     -5.24       |     -3.63       |      0.40       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -6.74       |     -4.63       |     -4.46       |\n",
      "| -7.34     -3.18 | -5.42      3.59 | -3.82     -1.42 |\n",
      "|     -3.02       |     -1.13       |      9.70       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -5.06       |     -3.86       |      9.63       |\n",
      "| -6.74     -1.92 | -4.77      9.68 |  9.66      9.68 |\n",
      "|     -5.29       |     -1.65       |      9.69       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "SARSA :optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†“         |      â†’         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†’         |      â†’         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TD(0) control : SARSA\n",
    "np.random.seed(0)\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,ğ‘)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : SARSA\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ì„œ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    temp =0\n",
    "    \n",
    "    # s ì—ì„œ í–‰ë™ ì •ì±…(Behavior policy)ìœ¼ë¡œ í–‰ë™ aë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "    action = e_greedy(Q_table,agent,epsilon)\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ì„œ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "#         Q_visit_count[action, pos[0],pos[1]] +=1\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s^'  ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' ì—ì„œ íƒ€ê¹ƒ ì •ì±…(Target policy)ìœ¼ë¡œ í–‰ë™ a^'ë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "        next_action = e_greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(ğ‘†,ğ´)â†Q(ğ‘†,ğ´) + Î±[ğ‘…+ğ›¾ğ‘„(ğ‘†',ğ´')âˆ’ğ‘„(ğ‘†,ğ´)]\n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "\n",
    "        # sâ†s^' ; aâ†a^'\n",
    "        action = next_action\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"SARSA : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"SARSA :optimal policy\")\n",
    "show_policy(optimal_policy,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) coltrol : Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TD(0) contro : Q-learning\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "np.random.seed(0)\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# ğ‘„(ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,ğ‘)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        # s ì—ì„œ í–‰ë™ ì •ì±…(Behavior policy)ìœ¼ë¡œ í–‰ë™ aë¥¼ ì„ íƒ ( ì˜ˆ : Îµ-greedy\n",
    "        pos = agent.get_pos()\n",
    "        action = e_greedy(Q_table,agent,epsilon)\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s^'ë¥¼ ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' ì—ì„œ íƒ€ê¹ƒ ì •ì±…(Target policy)ìœ¼ë¡œ í–‰ë™ a^'ë¥¼ ì„ íƒ ( ì˜ˆ : greedy)\n",
    "\n",
    "        next_action = greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(s,a)â†Q(s,a) + Î±[r+ğ›¾  maxa'ğ‘„(s',a')âˆ’ğ‘„(s,a)] \n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"Q-learning : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Double Q-learning with Îµ-greedy\n",
    "np.random.seed(0)\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# ëª¨ë“  ğ‘ âˆˆğ‘†,ğ‘âˆˆğ´(ğ‘†)ì— ëŒ€í•´ ì´ˆê¸°í™”:\n",
    "# ğ‘„1 (ğ‘ ,ğ‘),ğ‘„2 (ğ‘ ,ğ‘)â†ì„ì˜ì˜ ê°’\n",
    "Q1_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "Q2_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "# ğ‘„1 (ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,)=ğ‘„2 (ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘ğ‘™âˆ’ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’,)=0 \n",
    "Q1_table[2,2,:] = 0\n",
    "Q2_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 10\n",
    "\n",
    "print(\"start Double Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.3\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    \n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # ğ‘„1ê³¼ ğ‘„2ë¡œ ë¶€í„° aë¥¼ ì„ íƒ (ì˜ˆ : ğœ–âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦ in ğ‘„1+ğ‘„2)\n",
    "        Q = Q1_table + Q2_table\n",
    "        action = e_greedy(Q,agent,epsilon)\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s'ë¥¼ ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # Sâ€™ì—ì„œ Target policy í–‰ë™ Aâ€™ë¥¼ ì„ íƒ ( ì˜ˆ : ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)\n",
    "        p = np.random.random()\n",
    "        # í™•ë¥ ì´ 0.5ë³´ë‹¤ ì‘ë‹¤ë©´\n",
    "        if (p<0.5):\n",
    "            next_action = greedy(Q1_table,agent,epsilon)\n",
    "            # ğ‘„1 (ğ‘ ,ğ‘)â†ğ‘„1 (ğ‘ ,ğ‘)+ Î±[ğ‘…+ğ›¾ğ‘„2 (ğ‘†',argmaxaQ1(s',a)âˆ’ğ‘„1 (ğ‘ ,ğ‘)]        \n",
    "            Q1_table[pos[0],pos[1],action] += alpha * (reward + gamma*Q2_table[observation[0],observation[1],next_action] - Q1_table[pos[0],pos[1],action])\n",
    "        else:\n",
    "            next_action = greedy(Q2_table,agent,epsilon)\n",
    "            # ğ‘„2 (ğ‘ ,ğ‘)â†ğ‘„2 (ğ‘ ,ğ‘)+ Î±[ğ‘…+ğ›¾ğ‘„1 (ğ‘†',argmaxQ2(s',a)âˆ’âˆ’ğ‘„2 (ğ‘ ,ğ‘)] \n",
    "            Q2_table[pos[0],pos[1],action] += alpha * (reward + gamma*Q1_table[observation[0],observation[1],next_action] - Q2_table[pos[0],pos[1],action])\n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q1_table[i,j,:]+Q2_table[i,j,:])\n",
    "print(\"Double Q-learning : Q1(s,a)\")\n",
    "show_q_table(np.round(Q1_table,2),env)\n",
    "print(\"Double Q-learning : Q2(s,a)\")\n",
    "show_q_table(np.round(Q2_table,2),env)\n",
    "print(\"Double Q-learning : Q1(s,a)+Q2(s,a)\")\n",
    "show_q_table(np.round(Q1_table+Q2_table,2),env)\n",
    "print(\"Double Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ì•¡í„°-í¬ë¦¬í‹±ì•Œê³ ë¦¬ì¦˜\n",
    "np.random.seed(0)\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# p(ğ‘ ,ğ‘), ğ‘‰(ğ‘ )â†ì„ì˜ì˜ ê°’\n",
    "V = np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "policy = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "# í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ ë³€í™˜\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start Actor-Critic\")\n",
    "alpha = 0.1\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    # S ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        \n",
    "        # Actor : p(ğ‘ ,ğ‘)ë¡œ ë¶€í„° aë¥¼ ì„ íƒ ( ì˜ˆ :Gibbs softmax method)\n",
    "        pos = agent.get_pos()\n",
    "        \n",
    "        # Gibbs softmax method ë¡œ ì„ íƒë  í™•ë¥ ì„ ì¡°ì •\n",
    "        pr = np.zeros(4)\n",
    "        for i in range(len(agent.action)):\n",
    "            pr[i] = np.exp(policy[pos[0],pos[1],i])/np.sum(np.exp(policy[pos[0],pos[1],:]))\n",
    "                \n",
    "        # í–‰ë™ ì„ íƒ\n",
    "        action =  np.random.choice(range(0,len(agent.action)), p=pr)            \n",
    "        \n",
    "        # í–‰ë™ aë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s'ë¥¼ ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        \n",
    "        # Critic í•™ìŠµ\n",
    "        # Î´t=r(t+1)+Î³V(S(t+1) )-V(St)\n",
    "        td_error = reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]]\n",
    "        V[pos[0],pos[1]] += alpha * td_error\n",
    "\n",
    "        # Actor í•™ìŠµ :\n",
    "        # p(st,at)=p(st,at)-Î²Î´_t\n",
    "        policy[pos[0],pos[1],action] += td_error * 0.01\n",
    "        \n",
    "        # í™•ë¥ ì— ìŒìˆ˜ê°€ ìˆì„ê²½ìš° ì „ë¶€ ì–‘ìˆ˜ê°€ ë˜ë„ë¡ ë³´ì •\n",
    "        if np.min(policy[pos[0],pos[1],:]) < 0:\n",
    "            policy[pos[0],pos[1],:] -= np.min(policy[pos[0],pos[1],:])\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(policy[i,j,:])\n",
    "        \n",
    "print(\"Actor - Critic : V(s)\")\n",
    "show_v_table(np.round(V,2),env)\n",
    "print(\"Actor - Critic : policy(s,a)\")\n",
    "show_q_table(np.round(policy,2),env)\n",
    "print(\"Actor - Critic : optimal policy\")\n",
    "show_policy(optimal_policy,env)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) í•¨ìˆ˜ ê·¼ì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TD(0) í•¨ìˆ˜ ê·¼ì‚¬\n",
    "np.random.seed(1)\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#ì´ˆê¸°í™” : \n",
    "# ğ‘£(ğ‘ â”‚ğ’˜)â† ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜\n",
    "# wâ† í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "# w[0]+ w[1] * x1  + w[1] * x2\n",
    "w = np.random.rand(env.reward.shape[0])\n",
    "w -= 0.5\n",
    "\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        v_table[i,j] = w[0] + w[1] * i + w[2] * j\n",
    "\n",
    "        \n",
    "print(\"Before : TD(0) Function Approximation : v(s|w)\")\n",
    "print()\n",
    "print(\"Initial w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# ìµœëŒ€ ì—í”¼ì†Œë“œ, ì—í”¼ì†Œë“œì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "alpha = 0.01\n",
    "epsilon = 0.3 \n",
    "print(\"start Function Approximation TD(0) prediction\")\n",
    "\n",
    "# ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta =0\n",
    "    # s ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    temp =0\n",
    "    #  ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # ë¬´ì‘ìœ„ë¡œ í–‰ë™ ì„ íƒ\n",
    "        action = np.random.randint(0,len(agent.action))   \n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ s'ë¥¼ ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        now_v =0\n",
    "        next_v =0\n",
    "        # í˜„ì¬ìƒíƒœê°€ì¹˜í•¨ìˆ˜ì™€ ë‹¤ìŒ ìƒíƒœê°€ì¹˜í•¨ìˆ˜ë¥¼ ğ‘£(ğ‘ â”‚ğ’˜)ë¡œë¶€í„° ê³„ì‚°\n",
    "        now_v = w[0] + np.dot(w[1:],pos)\n",
    "        next_v = w[0] + np.dot(w[1:],observation)    \n",
    "        # wâ†ğ‘¤+ğ›¼[ğ‘Ÿâˆ’ğ‘£(ğ‘ â”‚ğ’˜)](ğœ•ğ‘£(sâ”‚ğ’˜))/ğœ•ğ‘¤\n",
    "        w[0] += alpha * ( reward + gamma * next_v - now_v )\n",
    "        w[1] += alpha * ( reward + gamma * next_v - now_v ) * pos[0]\n",
    "        w[2] += alpha * ( reward + gamma * next_v - now_v ) * pos[1]\n",
    "\n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        v_table[i,j] = w[0] + w[1] * i + w[2] * j\n",
    "\n",
    "print()        \n",
    "print(\"After : TD(0) Function Approximation : v(s|w)\")\n",
    "print()\n",
    "print(\"Final w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "\n",
    "print(\"TD(0) Function Approximation : V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning í•¨ìˆ˜ê·¼ì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 299/100000 [00:00<00:33, 2960.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : Function Approximation Q-learning : Q(s,a|w)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.05       |      0.15       |      0.25       |\n",
      "| -0.12      0.04 | -0.09      0.19 | -0.06      0.34 |\n",
      "|     -0.06       |      0.40       |      0.86       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.26       |      0.37       |      0.47       |\n",
      "|  0.18     -0.03 |  0.20      0.11 |  0.23      0.26 |\n",
      "|      0.33       |      0.79       |      1.26       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.48       |      0.58       |      0.68       |\n",
      "|  0.47     -0.11 |  0.50      0.04 |  0.52      0.18 |\n",
      "|      0.72       |      1.18       |      1.65       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Before : Function Approximation Q-learning :optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†‘         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      â†“         |      â†“         |      â†“         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Initial w\n",
      "w = [[ 0.05  0.22  0.1 ]\n",
      " [ 0.04 -0.08  0.15]\n",
      " [-0.06  0.39  0.46]\n",
      " [-0.12  0.29  0.03]]\n",
      "\n",
      "start Function Approximation : Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–        | 12808/100000 [00:07<00:49, 1762.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-31-11db7f29eb98>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     76\u001B[0m         \u001B[0mbest_action\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnext_act\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 78\u001B[1;33m         \u001B[0mnow_q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpos\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     79\u001B[0m         \u001B[0mnext_q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbest_action\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mobservation\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbest_action\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#  Q-learning í•¨ìˆ˜ê·¼ì‚¬\n",
    "np.random.seed(0)\n",
    "\n",
    "# í™˜ê²½, ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# ì´ˆê¸°í™” : \n",
    "# ğ‘(ğ‘ ,ğ‘â”‚ğ’˜)â† ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜\n",
    "# wâ† í•¨ìˆ˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "\n",
    "w = np.random.rand(len(agent.action),env.reward.shape[0])\n",
    "w -= 0.5\n",
    "\n",
    "FA_Q_table = np.zeros((env.reward.shape[0],env.reward.shape[1],len(agent.action)))\n",
    "# í•¨ìˆ˜ë¥¼ í…Œì´ë¸”ì— ì €ì¥\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        for k in range(len(agent.action)):\n",
    "            FA_Q_table[i,j,k] = w[k,0]  + w[k,1] * i + w[k,2]* j          \n",
    "\n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(FA_Q_table[i,j,:])\n",
    "\n",
    "\n",
    "print(\"Before : Function Approximation Q-learning : Q(s,a|w)\")\n",
    "show_q_table(np.round(FA_Q_table,2),env)\n",
    "print()\n",
    "print(\"Before : Function Approximation Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)  \n",
    "print()\n",
    "print(\"Initial w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "print()\n",
    "\n",
    "\n",
    "max_episode = 100000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start Function Approximation : Q-learning\")\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "#ê° ì—í”¼ì†Œë“œì— ëŒ€í•´ ë°˜ë³µ :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    # s ë¥¼ ì´ˆê¸°í™”\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # ì—í”¼ì†Œë“œì˜ ê° ìŠ¤í…ì— ëŒ€í•´ ë°˜ë³µ :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # sì—ì„œ Behavior policyë¡œ í–‰ë™ aë¥¼ ì„ íƒ (Gibbs ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì‚¬ìš©)\n",
    "        action = np.zeros(4)\n",
    "        for act in range(len(agent.action)):\n",
    "            action[act] = w[act,0]  + w[act,1]* pos[0] + w[act,2]*pos[1]\n",
    "            \n",
    "        pr = np.zeros(4)\n",
    "        for i in range(len(agent.action)):\n",
    "            pr[i] = np.exp(action[i])/np.sum(np.exp(action[:]))\n",
    "                \n",
    "        action = np.random.choice(range(0,len(agent.action)), p=pr)  \n",
    "\n",
    "        # í–‰ë™ a ë¥¼ ì·¨í•œ í›„ ë³´ìƒ rê³¼ ë‹¤ìŒ ìƒíƒœ sâ€™ë¥¼ ê´€ì¸¡\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        \n",
    "        # sâ€™ ì—ì„œ Target policy í–‰ë™ a'ë¥¼ ì„ íƒ (ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)\n",
    "        next_act = np.zeros(4)\n",
    "        for act in range(len(agent.action)):\n",
    "            next_act[act] = np.dot(w[act,1:],observation) + w[act,0]\n",
    "        best_action = np.argmax(next_act)\n",
    "        \n",
    "        now_q = np.dot(w[action,1:],pos) + w[action,0]\n",
    "        next_q = np.dot(w[best_action,1:],observation) + w[best_action,0]\n",
    "       \n",
    "        # w ê°±ì‹ \n",
    "        w[action,0] += alpha * (reward + gamma * next_q - now_q)\n",
    "        w[action,1] += alpha * (reward + gamma * next_q - now_q) * pos[0]\n",
    "        w[action,2] += alpha * (reward + gamma * next_q - now_q) * pos[1]\n",
    "        \n",
    "        # sê°€ ë§ˆì§€ë§‰ ìƒíƒœë¼ë©´ ì¢…ë£Œ\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "# FA_Q = np.zeros((len(agent.action),env.reward.shape[0],env.reward.shape[1]))\n",
    "# í•¨ìˆ˜ë¥¼ í…Œì´ë¸”ì— ì €ì¥\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        for k in range(len(agent.action)):\n",
    "            FA_Q_table[i,j,k] = w[k,0]  + w[k,1] * i + w[k,2]* j          \n",
    "\n",
    "# í•™ìŠµëœ ì •ì±…ì—ì„œ ìµœì  í–‰ë™ ì¶”ì¶œ\n",
    "# optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(FA_Q_table[i,j,:])\n",
    "\n",
    "    \n",
    "print(\"After : Function Approximation Q-learning : Q(s,a|w)\")\n",
    "show_q_table(np.round(FA_Q_table,2),env)\n",
    "print()\n",
    "print(\"After : Function Approximation Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)  \n",
    "print()\n",
    "print(\"Final w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}