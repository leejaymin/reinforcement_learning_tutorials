{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # progressbar \n",
    "import time\n",
    "import copy # copy를 실수 하지 않게. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기  (상태의 가치 총 9개 3x3)\n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기 (3x3x4) 엑션 가치 \n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 진행하는 주체 \n",
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Agent_jemin():\n",
    "\n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽)\n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "\n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "\n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "\n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "\n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Environment_jemin(): #환경 정의도 중요하다.\n",
    "\n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "\n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [4,4]\n",
    "\n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road,road,road],\n",
    "                   [road,road,road,road,road],\n",
    "                   [cliff,cliff,road,cliff,cliff],\n",
    "                   [road,road,road,road,road],\n",
    "                   [road,road,road,road,goal]]\n",
    "\n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\",\"road\",\"road\"],\n",
    "                    [\"cliff\",\"cliff\",\"road\",\"cliff\",\"cliff\"],\n",
    "                    [\"road\",\"road\",\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\",\"road\",\"goal\"]]\n",
    "\n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)\n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "\n",
    "        done = False\n",
    "\n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "\n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인 and cliff 인지 확인\n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1] :\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        elif self.reward_list1[new_pos[0]][new_pos[1]] == \"cliff\":\n",
    "            #print(\"cliff\")\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "\n",
    "        return observation, reward, done\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(): #환경 정의도 중요하다. \n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 상태 가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 가치 계산\n",
    "def state_value_function(env,agent,G,max_step,now_step):\n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "# 2. 현재 위치가 도착지점인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    \n",
    "# 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 3.1 가능한 모든 행동의 보상을 계산\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            observation, reward, done = env.move(agent,i)\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "            \n",
    "        return G # 마지막 상태의 값만 리턴한다.\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 step으로 이동\n",
    "    else:\n",
    "        \n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.2 현재 위치에서 가능한 모든 행동을 조사한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            observation, reward, done = env.move(agent,i)      \n",
    "            # 4.2.1 현재 상태에서 보상을 계산\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "\n",
    "            # 4.2.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "            if done == True:\n",
    "                if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                    agent.set_pos(pos1)\n",
    "\n",
    "            # 4.2.3 다음 step을 계산\n",
    "            # recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\n",
    "            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v # 취한 엑션, 디스카운트 팩터, 다음 엑션\n",
    "\n",
    "            # 4.2.4 현재 위치를 복구\n",
    "            agent.set_pos(pos1)\n",
    "\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미로의 각 상태의 상태가치함수를 구하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 0 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.50      |      -2.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -1.50      |      -1.00      |      -1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.00      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.58      |      -2.96      |      -3.46      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.96      |      -2.12      |      -1.68      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.46      |      -1.68      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 2 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.94      |      -4.23      |      -4.60      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.23      |      -3.09      |      -2.41      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.60      |      -2.41      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 3 total_time = 0.02(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.13      |      -5.29      |      -5.56      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.29      |      -3.99      |      -3.05      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.56      |      -3.05      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 4 total_time = 0.06(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.14      |      -6.22      |      -6.38      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.22      |      -4.75      |      -3.61      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.38      |      -3.61      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 5 total_time = 0.19(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.01      |      -7.01      |      -7.08      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.01      |      -5.42      |      -4.09      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.08      |      -4.09      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 6 total_time = 0.63(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.76      |      -7.69      |      -7.69      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -6.00      |      -4.51      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -4.51      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-12-a4c50312716e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     22\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreward\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m             \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_pos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mj\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m             \u001B[0mv_table\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mj\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;31m# 5.3 max_down에 따른 계산시간 저장\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 다음 step을 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# 취한 엑션, 디스카운트 팩터, 다음 엑션\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 다음 step을 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# 취한 엑션, 디스카운트 팩터, 다음 엑션\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 다음 step을 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# 취한 엑션, 디스카운트 팩터, 다음 엑션\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 다음 step을 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# 취한 엑션, 디스카운트 팩터, 다음 엑션\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 다음 step을 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# 취한 엑션, 디스카운트 팩터, 다음 엑션\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 다음 step을 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# 취한 엑션, 디스카운트 팩터, 다음 엑션\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# 4.2.3 다음 step을 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# recursive call. 끝나는 조건은 골에 도달하거나 끝일 때이다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mnext_v\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate_value_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnow_step\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnext_v\u001B[0m \u001B[1;31m# 취한 엑션, 디스카운트 팩터, 다음 엑션\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-11-c8f0295bd704>\u001B[0m in \u001B[0;36mstate_value_function\u001B[1;34m(env, agent, G, max_step, now_step)\u001B[0m\n\u001B[0;32m     17\u001B[0m             \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_pos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpos1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m             \u001B[0mobservation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmove\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0magent\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m             \u001B[0mG\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action_pr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mG\u001B[0m \u001B[1;31m# 마지막 상태의 값만 리턴한다.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "\n",
    "# 3. 최대 max_step_number 제한\n",
    "max_step_number = 13\n",
    "\n",
    "# 4. 계산 시간 저장을 위한 list\n",
    "time_len = []\n",
    "\n",
    "# 5. 재귀함수 state_value_function을를 이용해 각 상태 가치를 계산\n",
    "for max_step in range(max_step_number):\n",
    "    \n",
    "    # 5.1 미로 각 상태의 가치를 테이블 형식으로 저장\n",
    "    v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 5.2 미로의 각 상태에 대해 state_value_function() 을 이용해 가치를 계산한 후 테이블 형식으로 저장\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            agent.set_pos([i,j])\n",
    "            v_table[i,j] = state_value_function(env,agent, 0, max_step, 0)\n",
    "            \n",
    "    # 5.3 max_down에 따른 계산시간 저장\n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n",
    "    \n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 6. step 별 계산 시간 그래프 그리기    \n",
    "plt.plot(time_len, 'o-k')\n",
    "plt.xlabel('max_down')\n",
    "plt.ylabel('time(s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 행동가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 가치 함수\n",
    "def action_value_function(env, agent, act, G, max_step, now_step):   \n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 2. 현재 위치가 목적지인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "\n",
    "    # 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act]*reward\n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 행동과 함께 다음 step으로 이동\n",
    "    else:\n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act] * reward\n",
    "        \n",
    "        # 4.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "        if done == True:            \n",
    "            if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                agent.set_pos(pos1)\n",
    "            \n",
    "        # 4.3 현재 위치를 다시 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.4 현재 위치에서 가능한 모든 행동을 선택한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미로의 각 상태의 행동가치함수를 구하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 재귀적으로 행동의 가치를 계산\n",
    "\n",
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "# 3. 현재부터 max_step 까지 계산\n",
    "max_step_number = 8\n",
    "\n",
    "# 4. 모든 상태에 대해\n",
    "for max_step in range(max_step_number):\n",
    "    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\n",
    "    print(\"max_step = {}\".format(max_step))\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            # 4.2 모든 행동에 대해\n",
    "            for action in range(len(agent.action)):\n",
    "                # 4.2.1 에이전트의 위치를 초기화\n",
    "                agent.set_pos([i,j])\n",
    "                # 4.2.2 현재 위치에서 행동 가치를 계산\n",
    "                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n",
    "\n",
    "    q = np.round(q_table,2)\n",
    "    print(\"Q - table\")\n",
    "    show_q_table(q, env)\n",
    "    print(\"High actions Arrow\")\n",
    "    show_q_table_arrow(q,env)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반복 정책 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 반복 정책 평가\n",
    "np.random.seed(0)\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# 1. 모든 𝑠∈𝑆^에 대해서 배열 𝑉(𝑠)=0으로 초기화\n",
    "v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "\n",
    "print(\"start Iterative Policy Evaluation\")\n",
    "\n",
    "k = 1\n",
    "print()\n",
    "print(\"V0(S)   k = 0\")\n",
    "\n",
    "# 초기화된 V 테이블 출력\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 시작 시간 변수에 저장\n",
    "start_time = time.time()\n",
    "\n",
    "# 반복\n",
    "while(True):    \n",
    "    # 2. Δ←0\n",
    "    delta = 0\n",
    "    # 3. v←(𝑠)\n",
    "    # 계산전 가치를 저장\n",
    "    temp_v = copy.deepcopy(v_table)\n",
    "    # 4. 모든 𝑠∈𝑆에 대해 : \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            G = 0\n",
    "            # 5. 가능한 모든 행동으로 다음상태만 이용해 𝑉(𝑠) 계산\n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                \n",
    "#                 print(\"s({0}): {1:5s} : {2:0.2f} = {3:0.2f} *({4:0.2f} +  {5:0.2f} *  {6:0.2f})\".format(i*env.reward.shape[0]+j,dic[action],agent.select_action_pr[action] * (reward + gamma*V[observation[0],observation[1]]), agent.select_action_pr[action],reward,gamma,V[observation[0],observation[1]]))\n",
    "\n",
    "                G += agent.select_action_pr[action] * (reward + gamma*v_table[observation[0],observation[1]])                    \n",
    "\n",
    "#             print(\"V{2}({0}) :sum = {1:.2f}\".format(i*env.reward.shape[0]+j,total,k))\n",
    "#             print()\n",
    "            v_table[i,j] = G\n",
    "    # 6. ∆←max⁡(∆,|v−𝑉(𝑠)|)\n",
    "    # 계산전과 계산후의 가치 차이 계산\n",
    "    delta = np.max([delta, np.max(np.abs(temp_v-v_table))])\n",
    "    \n",
    "    end_time = time.time()        \n",
    "    print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f} total_time = {3}\".format(k,k, delta,np.round(end_time-start_time),2))\n",
    "    show_v_table(np.round(v_table,2),env)                \n",
    "    k +=1\n",
    "\n",
    "    # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n",
    "\n",
    "    if delta < 0.000001:\n",
    "        break\n",
    "        \n",
    "end_time = time.time()        \n",
    "print(\"total_time = {}\".format(np.round(end_time-start_time),2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 반복 정책 평가 (5by5, jemin)\n",
    "np.random.seed(0)\n",
    "env = Environment_jemin()\n",
    "agent = Agent_jemin()\n",
    "gamma = 0.99\n",
    "\n",
    "# 1. 모든 𝑠∈𝑆^에 대해서 배열 𝑉(𝑠)=0으로 초기화\n",
    "v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "\n",
    "print(\"start Iterative Policy Evaluation\")\n",
    "\n",
    "k = 1\n",
    "print()\n",
    "print(\"V0(S)   k = 0\")\n",
    "\n",
    "# 초기화된 V 테이블 출력\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 시작 시간 변수에 저장\n",
    "start_time = time.time()\n",
    "\n",
    "# 반복\n",
    "while(True):\n",
    "    # 2. Δ←0\n",
    "    delta = 0\n",
    "    # 3. v←(𝑠)\n",
    "    # 계산전 가치를 저장\n",
    "    temp_v = copy.deepcopy(v_table)\n",
    "    # 4. 모든 𝑠∈𝑆에 대해 :\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            G = 0\n",
    "            # 5. 가능한 모든 행동으로 다음상태만 이용해 𝑉(𝑠) 계산\n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "\n",
    "#                 print(\"s({0}): {1:5s} : {2:0.2f} = {3:0.2f} *({4:0.2f} +  {5:0.2f} *  {6:0.2f})\".format(i*env.reward.shape[0]+j,dic[action],agent.select_action_pr[action] * (reward + gamma*V[observation[0],observation[1]]), agent.select_action_pr[action],reward,gamma,V[observation[0],observation[1]]))\n",
    "\n",
    "                G += agent.select_action_pr[action] * (reward + gamma*v_table[observation[0],observation[1]])\n",
    "\n",
    "#             print(\"V{2}({0}) :sum = {1:.2f}\".format(i*env.reward.shape[0]+j,total,k))\n",
    "#             print()\n",
    "            v_table[i,j] = G\n",
    "    # 6. ∆←max⁡(∆,|v−𝑉(𝑠)|)\n",
    "    # 계산전과 계산후의 가치 차이 계산\n",
    "    delta = np.max([delta, np.max(np.abs(temp_v-v_table))])\n",
    "\n",
    "    end_time = time.time()\n",
    "    #print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f} total_time = {3}\".format(k,k, delta,np.round(end_time-start_time),2))\n",
    "    #show_v_table_small(np.round(v_table,2),env)\n",
    "    k +=1\n",
    "\n",
    "    # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n",
    "\n",
    "    if delta < 0.00000001:\n",
    "        show_v_table_small(np.round(v_table,2),env)\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"total_time = {}\".format(np.round(end_time-start_time),2))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random V(S)\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.55      |       0.72      |       0.60      |       0.54      |       0.42      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.65      |       0.44      |       0.89      |       0.96      |       0.38      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.79      |       0.53      |       0.57      |       0.93      |       0.07      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.09      |       0.02      |       0.83      |       0.78      |       0.87      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.98      |       0.80      |       0.46      |       0.78      |       0.12      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "Initial random Policy π0(S)\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      ↑         |      →         |      →         |      →         |      ←         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      ↑         |      ←         |      ↓         |      ↑         |      ←         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      ←         |      ↓         |      ←         |      ↓         |      ←         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      ↑         |      ↓         |      ↑         |      ↑         |      ↑         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |      ↑         |      ↑         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "start policy iteration\n",
      "\n",
      "Vπ0(S) delta = 0.0000009817\n",
      "\n",
      "policy π1(S)\n",
      "\n",
      "Vπ1(S) delta = 0.0000008809\n",
      "\n",
      "policy π2(S)\n",
      "\n",
      "Vπ2(S) delta = 0.0000000001\n",
      "\n",
      "policy π3(S)\n",
      "\n",
      "Vπ3(S) delta = 0.0000000001\n",
      "\n",
      "policy π4(S)\n",
      "\n",
      "Vπ4(S) delta = 0.0000000001\n",
      "\n",
      "policy π5(S)\n",
      "\n",
      "Vπ5(S) delta = 0.0000000000\n",
      "\n",
      "policy π6(S)\n",
      "\n",
      "Vπ6(S) delta = 0.0000000000\n",
      "\n",
      "policy π7(S)\n",
      "+----------+----------+----------+----------+----------\n",
      "|   -0.43  |    0.63  |    1.81  |    0.63  |   -0.43  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    0.63  |    1.81  |    3.12  |    1.81  |    0.63  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    1.81  |    3.12  |    4.58  |    6.20  |    8.00  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    3.12  |    4.58  |    6.20  |    8.00  |   10.00  |\n",
      "+----------+----------+----------+----------+----------\n",
      "|    4.58  |    6.20  |    8.00  |   10.00  |   10.00  |\n",
      "+----------+----------+----------+----------+----------\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |      ←         |      ←         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      ↓         |      →         |      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "total_time = 0.07819056510925293\n"
     ]
    }
   ],
   "source": [
    "# 숙제 1\n",
    "def policy_evalution(env, agent, v_table, policy):\n",
    "    gamma = 0.9\n",
    "    while(True):\n",
    "        # Δ←0\n",
    "        delta = 0\n",
    "        #  v←𝑉(𝑠)\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "        # 모든 𝑠∈𝑆에 대해 :\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                # 에이전트를 지정된 좌표에 위치시킨후 가치함수를 계산\n",
    "                agent.set_pos([i,j])\n",
    "                # 현재 정책의 행동을 선택\n",
    "                action = policy[i,j]\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                v_table[i,j] = reward + gamma * v_table[observation[0],observation[1]]\n",
    "        # ∆←max⁡(∆,|v−𝑉(𝑠)|)\n",
    "        # 계산전과 계산후의 가치의 차이를 계산\n",
    "        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n",
    "                \n",
    "        # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n",
    "        if delta < 0.000001:\n",
    "            break\n",
    "    return v_table, delta\n",
    "\n",
    "\n",
    "def policy_improvement(env, agent, v_table, policy):\n",
    "    \n",
    "    # 67페이지 아래 누락 되어있습니다\n",
    "    gamma = 0.9  \n",
    "    \n",
    "    # policyStable ← true \n",
    "    policyStable = True\n",
    "\n",
    "    # 모든 s∈S에 대해：\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):            \n",
    "            # 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛←π(s) \n",
    "            old_action = policy[i,j]            \n",
    "            # 가능한 행동중 최댓값을 가지는 행동을 선택\n",
    "            temp_action = 0\n",
    "            temp_value =  -1e+10           \n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent,action)\n",
    "                if temp_value < reward + gamma * v_table[observation[0],observation[1]]:\n",
    "                    temp_action = action\n",
    "                    temp_value = reward + gamma * v_table[observation[0],observation[1]]\n",
    "            # 만약 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛\"≠π(s)\"라면， \"policyStable ← False\" \n",
    "            # old-action과 새로운 action이 다른지 체크\n",
    "            if old_action != temp_action :\n",
    "                policyStable = False\n",
    "            policy[i,j] = temp_action\n",
    "    return policy, policyStable\n",
    "\n",
    "# 정책 반복\n",
    "# 환경과 에이전트에 대한 초기 설정\n",
    "np.random.seed(0)\n",
    "env = Environment_jemin()\n",
    "agent = Agent_jemin()\n",
    "\n",
    "# 1. 초기화\n",
    "# 모든 𝑠∈𝑆에 대해 𝑉(𝑠)∈𝑅과 π(𝑠)∈𝐴(𝑠)를 임의로 설정\n",
    "v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "policy = np.random.randint(0, 4,(env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "print(\"Initial random V(S)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()\n",
    "print(\"Initial random Policy π0(S)\")\n",
    "show_policy(policy,env)\n",
    "print(\"start policy iteration\")\n",
    "\n",
    "# 시작 시간을 변수에 저장\n",
    "start_time = time.time()\n",
    "\n",
    "max_iter_number = 20000\n",
    "for iter_number in range(max_iter_number):\n",
    "    \n",
    "    # 2.정책평가\n",
    "    v_table, delta = policy_evalution(env, agent, v_table, policy)\n",
    "\n",
    "    # 정책 평가 후 결과 표시                                            \n",
    "    print(\"\")\n",
    "    print(\"Vπ{0:}(S) delta = {1:.10f}\".format(iter_number,delta))\n",
    "    #show_v_table_small(np.round(v_table,2),env)\n",
    "    print()    \n",
    "    \n",
    "    \n",
    "    # 3.정책개선\n",
    "    policy, policyStable = policy_improvement(env, agent, v_table, policy)\n",
    "\n",
    "    # policy 변화 저장\n",
    "    print(\"policy π{}(S)\".format(iter_number+1))\n",
    "    #show_policy(policy,env)\n",
    "    # 하나라도 old-action과 새로운 action이 다르다면 '2. 정책평가'를 반복\n",
    "    if(policyStable == True):\n",
    "        show_v_table_small(np.round(v_table,2),env)\n",
    "        show_policy(policy,env)\n",
    "        break\n",
    "\n",
    "print(\"total_time = {}\".format(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가치반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial random V0(S)\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.55      |       0.72      |       0.60      |       0.54      |       0.42      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.65      |       0.44      |       0.89      |       0.96      |       0.38      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.79      |       0.53      |       0.57      |       0.93      |       0.07      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.09      |       0.02      |       0.83      |       0.78      |       0.87      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.98      |       0.80      |       0.46      |       0.78      |       0.12      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "start Value iteration\n",
      "\n",
      "V154(S) : k = 154    delta = 0.000000\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      -0.43      |       0.63      |       1.81      |       0.63      |      -0.43      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       0.63      |       1.81      |       3.12      |       1.81      |       0.63      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       1.81      |       3.12      |       4.58      |       6.20      |       8.00      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       3.12      |       4.58      |       6.20      |       8.00      |      10.00      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|       4.58      |       6.20      |       8.00      |      10.00      |      10.00      |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "total_time = 0.0\n",
      "\n",
      "Optimal policy\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |      ←         |      ←         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      ↓         |      →         |      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |                 |                 |\n",
      "|      →         |      →         |      →         |      →         |      ↑         |\n",
      "|                 |                 |                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "#숙제 2\n",
    "def finding_optimal_value_function(env, agent, v_table):\n",
    "    k = 1\n",
    "    gamma = 0.9\n",
    "    while(True):\n",
    "        # Δ←0\n",
    "        delta=0\n",
    "        #  v←𝑉(𝑠)\n",
    "        temp_v = copy.deepcopy(v_table)\n",
    "\n",
    "        # 모든 𝑠∈𝑆에 대해 :\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                temp = -1e+10\n",
    "#                 print(\"s({0}):\".format(i*env.reward.shape[0]+j))\n",
    "                # 𝑉(𝑠)← max(a)⁡∑𝑃(𝑠'|𝑠,𝑎)[𝑟(𝑠,𝑎,𝑠') +𝛾𝑉(𝑠')]\n",
    "                # 가능한 행동을 선택\n",
    "                for action in range(len(agent.action)):\n",
    "                    agent.set_pos([i,j])\n",
    "                    observation, reward, done = env.move(agent, action)\n",
    "#                     print(\"{0:.2f} = {1:.2f} + {2:.2f} * {3:.2f}\" .format(reward + gamma* v_table[observation[0],observation[1]],reward, gamma,v_table[observation[0],observation[1]]))\n",
    "                    #이동한 상태의 가치가 temp보다 크면\n",
    "                    if temp < reward + gamma*v_table[observation[0],observation[1]]:\n",
    "                        # temp 에 새로운 가치를 저장\n",
    "                        temp = reward + gamma*v_table[observation[0],observation[1]]  \n",
    "#                 print(\"V({0}) :max = {1:.2f}\".format(i*env.reward.shape[0]+j,temp))\n",
    "#                 print()\n",
    "                # 이동 가능한 상태 중 가장 큰 가치를 저장\n",
    "                v_table[i,j] = temp\n",
    "\n",
    "        #  ∆←max⁡(∆,|v−𝑉(𝑠)|)\n",
    "        # 이전 가치와 비교해서 큰 값을 delta에 저장\n",
    "        # 계산전과 계산후의 가치의 차이 계산\n",
    "        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n",
    "        # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n",
    "        if delta < 0.0000001:\n",
    "            print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n",
    "            show_v_table(np.round(v_table,2),env)\n",
    "            break\n",
    "            \n",
    "#         if k < 4 or k > 150:\n",
    "#             print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n",
    "#             show_v_table(np.round(v_table,2),env)\n",
    "        #print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n",
    "        #show_v_table(np.round(v_table,2),env)\n",
    "        k +=1\n",
    "        \n",
    "    return v_table\n",
    "\n",
    "def policy_extraction(env, agent, v_table, optimal_policy):\n",
    "\n",
    "    gamma = 0.9\n",
    "    \n",
    "    #정책 𝜋를 다음과 같이 추출\n",
    "    # 𝜋(𝑠)← argmax(a)⁡∑𝑃(𝑠'|𝑠,𝑎)[𝑟(𝑠,𝑎,𝑠') +𝛾𝑉(𝑠')]\n",
    "    # 모든 𝑠∈𝑆에 대해 : \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            temp =  -1e+10\n",
    "            # 가능한 행동중 가치가 가장높은 값을 policy[i,j]에 저장\n",
    "            for action in range(len(agent.action)):\n",
    "                agent.set_pos([i,j])\n",
    "                observation, reward, done = env.move(agent,action)\n",
    "                if temp < reward + gamma * v_table[observation[0],observation[1]]:\n",
    "                    optimal_policy[i,j] = action\n",
    "                    temp = reward + gamma * v_table[observation[0],observation[1]]\n",
    "                \n",
    "    return optimal_policy\n",
    "\n",
    "\n",
    "# 가치 반복\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "np.random.seed(0)\n",
    "env = Environment_jemin()\n",
    "agent = Agent_jemin()\n",
    "\n",
    "# 초기화\n",
    "# 모든 𝑠∈𝑆^+에 대해 𝑉(𝑠)∈𝑅을 임의로 설정\n",
    "v_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "\n",
    "print(\"Initial random V0(S)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()\n",
    "\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "print(\"start Value iteration\")\n",
    "print()\n",
    "\n",
    "# 시작 시간 변수에 저장\n",
    "start_time = time.time()\n",
    "\n",
    "v_table = finding_optimal_value_function(env, agent, v_table)\n",
    "\n",
    "optimal_policy = policy_extraction(env, agent, v_table, optimal_policy)\n",
    "\n",
    "                \n",
    "print(\"total_time = {}\".format(np.round(time.time()-start_time),2))\n",
    "print()\n",
    "print(\"Optimal policy\")\n",
    "show_policy(optimal_policy, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에피소드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent, first_visit):\n",
    "    gamma = 0.09\n",
    "    # 에피소드를 저장할 리스트\n",
    "    episode = []\n",
    "    # 이전에 방문여부 체크\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    \n",
    "    # 에이전트가 모든 상태에서 출발할 수 있게 출발지점을 무작위로 설정\n",
    "    i = np.random.randint(0,env.reward.shape[0])\n",
    "    j = np.random.randint(0,env.reward.shape[1])\n",
    "    agent.set_pos([i,j])    \n",
    "    #에피소드의 수익을 초기화\n",
    "    G = 0\n",
    "    #감쇄율의 지수\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    # 에피소드 생성\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()            \n",
    "        action = np.random.randint(0,len(agent.action))            \n",
    "        observaetion, reward, done = env.move(agent, action)    \n",
    "        \n",
    "        if first_visit:\n",
    "            # 에피소드에 첫 방문한 상태인지 검사 :\n",
    "            # visit[pos[0],pos[1]] == 0 : 첫 방문\n",
    "            # visit[pos[0],pos[1]] == 1 : 중복 방문\n",
    "            if visit[pos[0],pos[1]] == 0:   \n",
    "                # 에피소드가 끝날때까지 G를 계산\n",
    "                G += gamma**(step) * reward        \n",
    "                # 방문 이력 표시\n",
    "                visit[pos[0],pos[1]] = 1\n",
    "                step += 1               \n",
    "                # 방문 이력 저장(상태, 행동, 보상)\n",
    "                episode.append((pos,action, reward))\n",
    "        else:\n",
    "            G += gamma**(step) * reward\n",
    "            step += 1                   \n",
    "            episode.append((pos,action,reward))            \n",
    "\n",
    "        # 에피소드가 종료했다면 루프에서 탈출\n",
    "        if done == True:                \n",
    "            break        \n",
    "            \n",
    "    return i, j, G, episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-visit and Every-Visit MC Prediction\n",
    "- 해당 구현은 교제와 다르게 스타팅 포인트가 다른 경우에 대해서만 갱신한다.\n",
    "- 원래 의사코드는 이전에 발생한 것으로 갱신 해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first-visit MC and every-visit MC prediction\n",
    "np.random.seed(0)\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 임의의 상태 가치 함수𝑉\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 상태별로 에피소드 출발횟수를 저장하는 테이블\n",
    "v_start = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 상태별로 도착지점 도착횟수를 저장하는 테이블\n",
    "v_success = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해)\n",
    "Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# 최대 에피소드 수를 지정\n",
    "max_episode = 100000\n",
    "\n",
    "# first visit 를 사용할지 every visit를 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    \n",
    "    i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "    \n",
    "    # 수익 𝐺를 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)에 추가(append)\n",
    "    Return_s[i][j].append(G)\n",
    "    \n",
    "    # 에피소드 발생 횟수 계산\n",
    "    episode_count = len(Return_s[i][j])\n",
    "    # 상태별 발생한 수익의 총합 계산\n",
    "    total_G = np.sum(Return_s[i][j])\n",
    "    # 상태별 발생한 수익의 평균 계산\n",
    "    v_table[i,j] = total_G / episode_count\n",
    "    \n",
    "  # 도착지점에 도착(reward = 1)했는지 체크    \n",
    "    # episode[-1][2] : 에피소드 마지막 상태의 보상\n",
    "    if episode[-1][2] == 1:\n",
    "        v_success[i,j] += 1\n",
    "\n",
    "# 에피소드 출발 횟수 저장 \n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        v_start[i,j] = len(Return_s[i][j])\n",
    "        \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print(\"V_start_count(s)\")\n",
    "show_v_table(np.round(v_start,2),env)\n",
    "print(\"V_success_pr(s)\")\n",
    "show_v_table(np.round(v_success/v_start,2),env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental mean 을 이용하는 몬테카를로 Prediction 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incremental mean 을 이용하는 몬테카를로 Prediction 알고리즘\n",
    "np.random.seed(0)\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 임의의 상태 가치 함수𝑉\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 추가\n",
    "# 상태를 방문한 횟수를 저장하는 테이블\n",
    "v_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 삭제\n",
    "# # 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해) : \n",
    "# Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n",
    "\n",
    "# 최대 에피소드 수와 에피소드 최대 길이지정\n",
    "max_episode = 100000\n",
    "\n",
    "# first visit을 사용할지 every visit을 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    \n",
    "    i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "    \n",
    "    # 삭제\n",
    "    # 수익 𝐺를 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)에 추가(append)\n",
    "    #Return_s[i][j].append(G)\n",
    "    \n",
    "    # 에피소드 발생 횟수 계산\n",
    "    #episode_count = len(Return_s[i][j])\n",
    "    # 상태별 발생한 수익의 총합 계산\n",
    "    #total_G = np.sum(Return_s[i][j])\n",
    "    # 상태별 발생한 수익의 평균 계산\n",
    "    #v_table[i,j] = total_G / episode_count\n",
    "    \n",
    "    #Return_length[i][j].append(len(episode))\n",
    "\n",
    "    #if episode[-1][2] == 1:\n",
    "    #    v_success[i,j] += 1    \n",
    "    \n",
    "    # Incremental mean  평균을 계산\n",
    "    v_visit[i,j] += 1\n",
    "    v_table[i,j] += 1 / v_visit[i,j] * (G - v_table[i,j])\n",
    "    \n",
    "      \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  에피소드 분리를 이용하는 Firsit-visit 몬테카를로 방법과 Every-visit 몬테카를로 방법의 Prediction 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent, first_visit):\n",
    "    gamma = 0.09\n",
    "    # 에피소드를 저장할 리스트\n",
    "    episode = []\n",
    "    # 이전에 방문여부 체크\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "    \n",
    "    # 에이전트는 항상 (0,0)에서 출발\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])    \n",
    "    #에피소드의 수익을 초기화\n",
    "    G = 0\n",
    "    #감쇄율의 지수\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    # 에피소드 생성\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()            \n",
    "        action = np.random.randint(0,len(agent.action))            \n",
    "        observaetion, reward, done = env.move(agent, action)    \n",
    "        \n",
    "        if first_visit:\n",
    "            # 에피소드에 첫 방문한 상태인지 검사 :\n",
    "            # visit[pos[0],pos[1]] == 0 : 첫 방문\n",
    "            # visit[pos[0],pos[1]] == 1 : 중복 방문\n",
    "            if visit[pos[0],pos[1]] == 0:   \n",
    "                # 에피소드가 끝날때까지 G를 계산\n",
    "                G += gamma**(step) * reward        \n",
    "                # 방문 이력 표시\n",
    "                visit[pos[0],pos[1]] = 1\n",
    "                step += 1               \n",
    "                # 방문 이력 저장(상태, 행동, 보상)\n",
    "                episode.append((pos,action, reward))\n",
    "        else:\n",
    "            G += gamma**(step) * reward\n",
    "            step += 1                   \n",
    "            episode.append((pos,action, reward))            \n",
    "\n",
    "        # 에피소드가 종료했다면 루프에서 탈출\n",
    "        if done == True:                \n",
    "            break        \n",
    "            \n",
    "    return i, j, G, episode\n",
    "\n",
    "#에피소드 분리를 이용하는 몬테카를로 Prediction 알고리즘\n",
    "np.random.seed(0)\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 임의의 상태 가치 함수𝑉\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 상태를 방문한 횟수를 저장하는 테이블\n",
    "v_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 최대 에피소드 수와 에피소드 최대 길이를 지정\n",
    "max_episode = 99999\n",
    "\n",
    "# first visit 를 사용할지 every visit를 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "gamma = 0.09\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    \n",
    "    i,j,G,episode = generate_episode(env, agent, first_visit)\n",
    "    for step_num in range(len(episode)):\n",
    "        G = 0\n",
    "        # episode[step_num][0][0] : step_num번째 방문한 상태의 x 좌표\n",
    "        # episode[step_num][0][1] : step_num번째 방문한 상태의 y 좌표\n",
    "        # episode[step_num][1] : step_num번째 상태에서 선택한 행동\n",
    "        i = episode[step_num][0][0]\n",
    "        j = episode[step_num][0][1]\n",
    "        # 에피소드 시작점을 카운트\n",
    "        v_visit[i,j] += 1\n",
    "\n",
    "        # 서브 에피소드 (episode[step_num:])의 출발부터 끝까지 보수 G를 계산\n",
    "        # k[2] : episode[step_num][2] 과 같으며 step_num 번째 받은 보상\n",
    "        # step : 감쇄율\n",
    "        for step, k in enumerate(episode[step_num:]):\n",
    "            G += gamma**(step)*k[2]\n",
    "            \n",
    "        # Incremental mean  평균을 계산\n",
    "        v_table[i,j] += 1 / v_visit[i,j] * (G - v_table[i,j])\n",
    "           \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "print(\"V_start_count(s)\")\n",
    "show_v_table(np.round(v_visit,2),env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e-greedy 확률 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episilon |   a1  |    a2   |    a3  |   a4  \n",
      "1.0      | 0.250 |  0.250  |  0.250 |  0.250\n",
      "0.9      | 0.225 |  0.225  |  0.225 |  0.325\n",
      "0.8      | 0.200 |  0.200  |  0.200 |  0.400\n",
      "0.7      | 0.175 |  0.175  |  0.175 |  0.475\n",
      "0.6      | 0.150 |  0.150  |  0.150 |  0.550\n",
      "0.5      | 0.125 |  0.125  |  0.125 |  0.625\n",
      "0.4      | 0.100 |  0.100  |  0.100 |  0.700\n",
      "0.3      | 0.075 |  0.075  |  0.075 |  0.775\n",
      "0.2      | 0.050 |  0.050  |  0.050 |  0.850\n",
      "0.1      | 0.025 |  0.025  |  0.025 |  0.925\n",
      "0.0      | 0.000 |  0.000  |  0.000 |  1.000\n"
     ]
    }
   ],
   "source": [
    "policy = np.array((0.1,0.2,0.3,0.4))\n",
    "up_list=[]\n",
    "right_list=[]\n",
    "down_list=[]\n",
    "left_list=[]\n",
    "print(\"episilon |   a1  |    a2   |    a3  |   a4  \")\n",
    "for epsilon in reversed(range(0, 11)):\n",
    "    epsilon /= 10\n",
    "    up = epsilon / len(policy)\n",
    "    right = epsilon / len(policy)\n",
    "    down = epsilon / len(policy)\n",
    "    left = 1 -  epsilon + epsilon / len(policy)\n",
    "    print(\"{0:}      | {1:.3f} |  {2:.3f}  |  {3:.3f} |  {4:.3f}\".format(np.round(epsilon,3),np.round(up,3),np.round(right,3),np.round(down,3),np.round(left,3)))\n",
    "#     policy_list.append((up,right,down,left))\n",
    "#     up_list.append(up)\n",
    "#     right_list.append(right)\n",
    "#     down_list.append(down)\n",
    "#     left_list.append(left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ϵ-정책을 이용하는 몬테카를로 control 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_episode_with_policy(env, agent, first_visit, policy):\n",
    "    gamma = 0.09\n",
    "    # 에피소드를 저장할 리스트\n",
    "    episode = []\n",
    "    # 이전에 방문여부 체크\n",
    "    visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    \n",
    "    # 에이전트는 항상 (0,0)에서 출발\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])    \n",
    "    #에피소드의 수익을 초기화\n",
    "    G = 0\n",
    "    #감쇄율의 지수\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    # 에피소드 생성\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()        \n",
    "        # 현재 상태의 정책을 이용해 행동을 선택한 후 이동\n",
    "        action = np.random.choice(range(0,len(agent.action)), p=policy[pos[0],pos[1],:]) \n",
    "        observaetion, reward, done = env.move(agent, action)    \n",
    "        \n",
    "        if first_visit:\n",
    "            # 에피소드에 첫 방문한 상태인지 검사 :\n",
    "            # visit[pos[0],pos[1]] == 0 : 첫 방문\n",
    "            # visit[pos[0],pos[1]] == 1 : 중복 방문\n",
    "            if visit[pos[0],pos[1],action] == 0:   \n",
    "                # 에피소드가 끝날때까지 G를 계산\n",
    "                G += gamma**(step) * reward        \n",
    "                # 방문 이력 표시\n",
    "                visit[pos[0],pos[1],action] = 1\n",
    "                step += 1               \n",
    "                # 방문 이력 저장(상태, 행동, 보상)\n",
    "                episode.append((pos,action, reward))\n",
    "        else:\n",
    "            G += gamma**(step) * reward\n",
    "            step += 1                   \n",
    "            episode.append((pos,action,reward))            \n",
    "\n",
    "        # 에피소드가 종료했다면 루프에서 탈출\n",
    "        if done == True:                \n",
    "            break        \n",
    "            \n",
    "    return i, j, G, episode\n",
    "\n",
    "# ϵ-정책을 이용하는 몬테카를로 control 알고리즘\n",
    "np.random.seed(0)\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n",
    "# # 𝑄(𝑠,𝑎)←임의의 값 (행동 개수, 미로 세로, 미로 가로)\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "print(\"Initial Q(s,a)\")\n",
    "show_q_table(Q_table,env)\n",
    "\n",
    "# 상태를 방문한 횟수를 저장하는 테이블\n",
    "Q_visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "# 미로 모든 상태에서 최적 행동을 저장하는 테이블\n",
    "# 각 상태에서 Q 값이 가장 큰 행동을 선택 후 optimal_a 에 저장\n",
    "optimal_a = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_a[i,j] = np.argmax(Q_table[i,j,:])\n",
    "print(\"initial optimal_a\")\n",
    "show_policy(optimal_a,env)\n",
    "\n",
    "# π(𝑠,𝑎)←임의의 𝜖−탐욕 정책\n",
    "# 무작위로 행동을 선택하도록 지정\n",
    "policy = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "# 한 상태에서 가능한 확률의 합이 1이 되도록 계산\n",
    "epsilon = 0.8\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        for k in range(len(agent.action)):\n",
    "            if optimal_a[i,j] == k:\n",
    "                policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n",
    "            else:\n",
    "                policy[i,j,k] = epsilon/len(agent.action)\n",
    "print(\"Initial Policy\")\n",
    "show_q_table(policy,env)\n",
    "\n",
    "\n",
    "# 최대 에피소드 수 길이를 지정\n",
    "max_episode = 10000\n",
    "\n",
    "# first visit 를 사용할지 every visit를 사용할 지 결정\n",
    "# first_visit = True : first visit\n",
    "# first_visit = False : every visit\n",
    "first_visit = True\n",
    "if first_visit:\n",
    "    print(\"start first visit MC\")\n",
    "else : \n",
    "    print(\"start every visit MC\")\n",
    "print()\n",
    "\n",
    "gamma = 0.09\n",
    "for epi in tqdm(range(max_episode)):\n",
    "# for epi in range(max_episode):\n",
    "\n",
    "    # π를 이용해서 에피소드 1개를 생성\n",
    "    x,y,G,episode = generate_episode_with_policy(env, agent, first_visit, policy)\n",
    "    \n",
    "    for step_num in range(len(episode)):\n",
    "        G = 0\n",
    "        # episode[step_num][0][0] : step_num번째 방문한 상태의 x 좌표\n",
    "        # episode[step_num][0][1] : step_num번째 방문한 상태의 y 좌표\n",
    "        # episode[step_num][1] : step_num번째 상태에서 선택한 행동\n",
    "        i = episode[step_num][0][0]\n",
    "        j = episode[step_num][0][1]\n",
    "        action = episode[step_num][1]\n",
    "        \n",
    "        # 에피소드 시작점을 카운트\n",
    "        Q_visit[i,j,action] += 1\n",
    "\n",
    "        # 서브 에피소드 (episode[step_num:])의 출발부터 끝까지 수익 G를 계산\n",
    "        # k[2] : episode[step_num][2] 과 같으며 step_num 번째 받은 보상\n",
    "        # step : 감쇄율\n",
    "        for step, k in enumerate(episode[step_num:]):\n",
    "            G += gamma**(step)*k[2]\n",
    "\n",
    "        # Incremental mean : 𝑄(𝑠,𝑎)←𝑎𝑣𝑒𝑟𝑎𝑔𝑒(𝑅𝑒𝑡𝑢𝑟𝑛(𝑠,𝑎)) \n",
    "        Q_table[i,j,action] += 1 / Q_visit[i,j,action]*(G-Q_table[i,j,action])\n",
    "    \n",
    "    # (c) 에피소드 안의 각 s에 대해서 :\n",
    "    # 미로 모든 상태에서 최적 행동을 저장할 공간 마련\n",
    "    # 𝑎∗ ←argmax_a 𝑄(𝑠,𝑎)\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            optimal_a[i,j] = np.argmax(Q_table[i,j,:])            \n",
    "   \n",
    "    # 모든 𝑎∈𝐴(𝑆) 에 대해서 :\n",
    "    # 새로 계산된 optimal_a 를 이용해서 행동 선택 확률 policy (π) 갱신\n",
    "    epsilon = 1 - epi/max_episode\n",
    "\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            for k in range(len(agent.action)):\n",
    "                if optimal_a[i,j] == k:\n",
    "                    policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n",
    "                else:\n",
    "                    policy[i,j,k] = epsilon/len(agent.action)\n",
    "\n",
    "print(\"Final Q(s,a)\")\n",
    "show_q_table(Q_table,env)\n",
    "print(\"Final policy\")\n",
    "show_q_table(policy,env)\n",
    "print(\"Final optimal_a\")\n",
    "show_policy(optimal_a,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD(0) prediction\n",
    "np.random.seed(0)\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#초기화 : \n",
    "#π← 평가할 정책\n",
    "# 가능한 모든 행동이 무작위로 선택되도록 지정\n",
    "#𝑉← 임의의 상태가치 함수\n",
    "V = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "\n",
    "# 최대 에피소드, 에피소드의 최대 길이를 지정\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "print(\"start TD(0) prediction\")\n",
    "\n",
    "# 각 에피소드에 대해 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta =0\n",
    "    # s 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    #  에피소드의 각 스텝에 대해 반복 :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # a←상태 𝑠 에서 정책 π에 의해 결정된 행동 \n",
    "        # 가능한 모든 행동이 무작위로 선택되게 함\n",
    "        action = np.random.randint(0,4)\n",
    "        # 행동 a 를 취한 후 보수 r과 다음 상태 s’를 관측\n",
    "        # s←𝑠'\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        # V(𝑠)←V(𝑠)+ α[𝑟+𝛾𝑉(𝑠^)−𝑉(𝑠)]\n",
    "        V[pos[0],pos[1]] += alpha * (reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]])\n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "print(\"V(s)\")\n",
    "show_v_table(np.round(V,2),env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 𝜖−𝑔𝑟𝑒𝑒𝑑𝑦, 𝑔𝑟𝑒𝑒𝑑𝑦  함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  𝜖−𝑔𝑟𝑒𝑒𝑑𝑦, 𝑔𝑟𝑒𝑒𝑑𝑦  함수 작성\n",
    "def e_greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    greedy_action = np.argmax(Q_table[pos[0],pos[1],:])\n",
    "    pr = np.zeros(4)\n",
    "    for i in range(len(agent.action)):\n",
    "        if i == greedy_action:\n",
    "            pr[i] = 1 - epsilon + epsilon/len(agent.action)\n",
    "        else:\n",
    "            pr[i] = epsilon / len(agent.action)\n",
    "\n",
    "    return np.random.choice(range(0,len(agent.action)), p=pr)    \n",
    "\n",
    "def greedy(Q_table,agent,epsilon):\n",
    "    pos = agent.get_pos()\n",
    "    return np.argmax(Q_table[pos[0],pos[1],:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) coltrol : SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 965/10000 [00:00<00:01, 4719.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start TD(0) control : SARSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5115.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSA : Q(s,a)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -9.20       |     -7.30       |     -5.16       |\n",
      "| -9.71     -5.47 | -7.55     -4.38 | -5.91     -6.55 |\n",
      "|     -5.24       |     -3.63       |      0.40       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -6.74       |     -4.63       |     -4.46       |\n",
      "| -7.34     -3.18 | -5.42      3.59 | -3.82     -1.42 |\n",
      "|     -3.02       |     -1.13       |      9.70       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -5.06       |     -3.86       |      9.63       |\n",
      "| -6.74     -1.92 | -4.77      9.68 |  9.66      9.68 |\n",
      "|     -5.29       |     -1.65       |      9.69       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "SARSA :optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      →         |      →         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TD(0) control : SARSA\n",
    "np.random.seed(0)\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n",
    "# 𝑄(𝑠,𝑎)←임의의 값\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,𝑎)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : SARSA\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# 각 에피소드에 대해서 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    # S 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    temp =0\n",
    "    \n",
    "    # s 에서 행동 정책(Behavior policy)으로 행동 a를 선택 ( 예 : ε-greedy\n",
    "    action = e_greedy(Q_table,agent,epsilon)\n",
    "\n",
    "    # 에피소드의 각 스텝에 대해서 반복 :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "#         Q_visit_count[action, pos[0],pos[1]] +=1\n",
    "        # 행동 a 를 취한 후 보상 r과 다음 상태 s^'  관측\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' 에서 타깃 정책(Target policy)으로 행동 a^'를 선택 ( 예 : ε-greedy\n",
    "        next_action = e_greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(𝑆,𝐴)←Q(𝑆,𝐴) + α[𝑅+𝛾𝑄(𝑆',𝐴')−𝑄(𝑆,𝐴)]\n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "\n",
    "        # s←s^' ; a←a^'\n",
    "        action = next_action\n",
    "        \n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# 학습된 정책에서 최적 행동 추출\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"SARSA : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"SARSA :optimal policy\")\n",
    "show_policy(optimal_policy,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) coltrol : Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TD(0) contro : Q-learning\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "np.random.seed(0)\n",
    "\n",
    "# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n",
    "# 𝑄(𝑠,𝑎)←임의의 값\n",
    "Q_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "\n",
    "# Q(𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,𝑎)=0\n",
    "Q_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start TD(0) control : Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "\n",
    "# 각 에피소드에 대해 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    dleta = 0\n",
    "    # S 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # 에피소드의 각 스텝에 대해 반복 :\n",
    "    for k in range(max_step):\n",
    "        # s 에서 행동 정책(Behavior policy)으로 행동 a를 선택 ( 예 : ε-greedy\n",
    "        pos = agent.get_pos()\n",
    "        action = e_greedy(Q_table,agent,epsilon)\n",
    "        # 행동 a 를 취한 후 보상 r과 다음 상태 s^'를 관측\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # s^' 에서 타깃 정책(Target policy)으로 행동 a^'를 선택 ( 예 : greedy)\n",
    "\n",
    "        next_action = greedy(Q_table,agent,epsilon)\n",
    "        \n",
    "        # Q(s,a)←Q(s,a) + α[r+𝛾  maxa'𝑄(s',a')−𝑄(s,a)] \n",
    "        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# 학습된 정책에서 최적 행동 추출\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n",
    "\n",
    "print(\"Q-learning : Q(s,a)\")\n",
    "show_q_table(np.round(Q_table,2),env)\n",
    "print(\"Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Double Q-learning with ε-greedy\n",
    "np.random.seed(0)\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n",
    "# 𝑄1 (𝑠,𝑎),𝑄2 (𝑠,𝑎)←임의의 값\n",
    "Q1_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "Q2_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "# 𝑄1 (𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,)=𝑄2 (𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,)=0 \n",
    "Q1_table[2,2,:] = 0\n",
    "Q2_table[2,2,:] = 0\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 10\n",
    "\n",
    "print(\"start Double Q-learning\")\n",
    "alpha = 0.1\n",
    "epsilon = 0.3\n",
    "\n",
    "# 각 에피소드에 대해 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    # S 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    \n",
    "    # 에피소드의 각 스텝에 대해 반복 :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # 𝑄1과 𝑄2로 부터 a를 선택 (예 : 𝜖−𝑔𝑟𝑒𝑒𝑑𝑦 in 𝑄1+𝑄2)\n",
    "        Q = Q1_table + Q2_table\n",
    "        action = e_greedy(Q,agent,epsilon)\n",
    "        # 행동 a 를 취한 후 보상 r과 다음 상태 s'를 관측\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "\n",
    "        # S’에서 Target policy 행동 A’를 선택 ( 예 : 𝑔𝑟𝑒𝑒𝑑𝑦)\n",
    "        p = np.random.random()\n",
    "        # 확률이 0.5보다 작다면\n",
    "        if (p<0.5):\n",
    "            next_action = greedy(Q1_table,agent,epsilon)\n",
    "            # 𝑄1 (𝑠,𝑎)←𝑄1 (𝑠,𝑎)+ α[𝑅+𝛾𝑄2 (𝑆',argmaxaQ1(s',a)−𝑄1 (𝑠,𝑎)]        \n",
    "            Q1_table[pos[0],pos[1],action] += alpha * (reward + gamma*Q2_table[observation[0],observation[1],next_action] - Q1_table[pos[0],pos[1],action])\n",
    "        else:\n",
    "            next_action = greedy(Q2_table,agent,epsilon)\n",
    "            # 𝑄2 (𝑠,𝑎)←𝑄2 (𝑠,𝑎)+ α[𝑅+𝛾𝑄1 (𝑆',argmaxQ2(s',a)−−𝑄2 (𝑠,𝑎)] \n",
    "            Q2_table[pos[0],pos[1],action] += alpha * (reward + gamma*Q1_table[observation[0],observation[1],next_action] - Q2_table[pos[0],pos[1],action])\n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "        \n",
    "# 학습된 정책에서 최적 행동 추출\n",
    "optimal_policy = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(Q1_table[i,j,:]+Q2_table[i,j,:])\n",
    "print(\"Double Q-learning : Q1(s,a)\")\n",
    "show_q_table(np.round(Q1_table,2),env)\n",
    "print(\"Double Q-learning : Q2(s,a)\")\n",
    "show_q_table(np.round(Q2_table,2),env)\n",
    "print(\"Double Q-learning : Q1(s,a)+Q2(s,a)\")\n",
    "show_q_table(np.round(Q1_table+Q2_table,2),env)\n",
    "print(\"Double Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  액터-크리틱알고리즘\n",
    "np.random.seed(0)\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# p(𝑠,𝑎), 𝑉(𝑠)←임의의 값\n",
    "V = np.random.rand(env.reward.shape[0], env.reward.shape[1])\n",
    "policy = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n",
    "# 확률의 합이 1이 되도록 변환\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])\n",
    "\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start Actor-Critic\")\n",
    "alpha = 0.1\n",
    "\n",
    "# 각 에피소드에 대해 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    # S 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # 에피소드의 각 스텝에 대해 반복 :\n",
    "    for k in range(max_step):\n",
    "        \n",
    "        # Actor : p(𝑠,𝑎)로 부터 a를 선택 ( 예 :Gibbs softmax method)\n",
    "        pos = agent.get_pos()\n",
    "        \n",
    "        # Gibbs softmax method 로 선택될 확률을 조정\n",
    "        pr = np.zeros(4)\n",
    "        for i in range(len(agent.action)):\n",
    "            pr[i] = np.exp(policy[pos[0],pos[1],i])/np.sum(np.exp(policy[pos[0],pos[1],:]))\n",
    "                \n",
    "        # 행동 선택\n",
    "        action =  np.random.choice(range(0,len(agent.action)), p=pr)            \n",
    "        \n",
    "        # 행동 a를 취한 후 보상 r과 다음 상태 s'를 관측\n",
    "        observation, reward, done = env.move(agent, action)\n",
    "        \n",
    "        # Critic 학습\n",
    "        # δt=r(t+1)+γV(S(t+1) )-V(St)\n",
    "        td_error = reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]]\n",
    "        V[pos[0],pos[1]] += alpha * td_error\n",
    "\n",
    "        # Actor 학습 :\n",
    "        # p(st,at)=p(st,at)-βδ_t\n",
    "        policy[pos[0],pos[1],action] += td_error * 0.01\n",
    "        \n",
    "        # 확률에 음수가 있을경우 전부 양수가 되도록 보정\n",
    "        if np.min(policy[pos[0],pos[1],:]) < 0:\n",
    "            policy[pos[0],pos[1],:] -= np.min(policy[pos[0],pos[1],:])\n",
    "        for i in range(env.reward.shape[0]):\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])\n",
    "        \n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "# 학습된 정책에서 최적 행동 추출\n",
    "optimal_policy = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(policy[i,j,:])\n",
    "        \n",
    "print(\"Actor - Critic : V(s)\")\n",
    "show_v_table(np.round(V,2),env)\n",
    "print(\"Actor - Critic : policy(s,a)\")\n",
    "show_q_table(np.round(policy,2),env)\n",
    "print(\"Actor - Critic : optimal policy\")\n",
    "show_policy(optimal_policy,env)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) 함수 근사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TD(0) 함수 근사\n",
    "np.random.seed(1)\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "#초기화 : \n",
    "# 𝑣(𝑠│𝒘)← 미분 가능한 함수\n",
    "# w← 함수의 가중치를 임의의 값으로 초기화\n",
    "# w[0]+ w[1] * x1  + w[1] * x2\n",
    "w = np.random.rand(env.reward.shape[0])\n",
    "w -= 0.5\n",
    "\n",
    "v_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        v_table[i,j] = w[0] + w[1] * i + w[2] * j\n",
    "\n",
    "        \n",
    "print(\"Before : TD(0) Function Approximation : v(s|w)\")\n",
    "print()\n",
    "print(\"Initial w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 최대 에피소드, 에피소드의 최대 길이를 지정\n",
    "max_episode = 10000\n",
    "max_step = 100\n",
    "alpha = 0.01\n",
    "epsilon = 0.3 \n",
    "print(\"start Function Approximation TD(0) prediction\")\n",
    "\n",
    "# 각 에피소드에 대해 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    delta =0\n",
    "    # s 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "    temp =0\n",
    "    #  에피소드의 각 스텝에 대해 반복 :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # 무작위로 행동 선택\n",
    "        action = np.random.randint(0,len(agent.action))   \n",
    "        # 행동 a 를 취한 후 보상 r과 다음 상태 s'를 관측\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        now_v =0\n",
    "        next_v =0\n",
    "        # 현재상태가치함수와 다음 상태가치함수를 𝑣(𝑠│𝒘)로부터 계산\n",
    "        now_v = w[0] + np.dot(w[1:],pos)\n",
    "        next_v = w[0] + np.dot(w[1:],observation)    \n",
    "        # w←𝑤+𝛼[𝑟−𝑣(𝑠│𝒘)](𝜕𝑣(s│𝒘))/𝜕𝑤\n",
    "        w[0] += alpha * ( reward + gamma * next_v - now_v )\n",
    "        w[1] += alpha * ( reward + gamma * next_v - now_v ) * pos[0]\n",
    "        w[2] += alpha * ( reward + gamma * next_v - now_v ) * pos[1]\n",
    "\n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        v_table[i,j] = w[0] + w[1] * i + w[2] * j\n",
    "\n",
    "print()        \n",
    "print(\"After : TD(0) Function Approximation : v(s|w)\")\n",
    "print()\n",
    "print(\"Final w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "\n",
    "print(\"TD(0) Function Approximation : V(s)\")\n",
    "show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning 함수근사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 299/100000 [00:00<00:33, 2960.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : Function Approximation Q-learning : Q(s,a|w)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.05       |      0.15       |      0.25       |\n",
      "| -0.12      0.04 | -0.09      0.19 | -0.06      0.34 |\n",
      "|     -0.06       |      0.40       |      0.86       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.26       |      0.37       |      0.47       |\n",
      "|  0.18     -0.03 |  0.20      0.11 |  0.23      0.26 |\n",
      "|      0.33       |      0.79       |      1.26       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|      0.48       |      0.58       |      0.68       |\n",
      "|  0.47     -0.11 |  0.50      0.04 |  0.52      0.18 |\n",
      "|      0.72       |      1.18       |      1.65       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Before : Function Approximation Q-learning :optimal policy\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↑         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      ↓         |      ↓         |      ↓         |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "Initial w\n",
      "w = [[ 0.05  0.22  0.1 ]\n",
      " [ 0.04 -0.08  0.15]\n",
      " [-0.06  0.39  0.46]\n",
      " [-0.12  0.29  0.03]]\n",
      "\n",
      "start Function Approximation : Q-learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12808/100000 [00:07<00:49, 1762.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-31-11db7f29eb98>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     76\u001B[0m         \u001B[0mbest_action\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnext_act\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 78\u001B[1;33m         \u001B[0mnow_q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpos\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     79\u001B[0m         \u001B[0mnext_q\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbest_action\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mobservation\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbest_action\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#  Q-learning 함수근사\n",
    "np.random.seed(0)\n",
    "\n",
    "# 환경, 에이전트를 초기화\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "gamma = 0.9\n",
    "\n",
    "# 초기화 : \n",
    "# 𝑞(𝑠,𝑎│𝒘)← 미분 가능한 함수\n",
    "# w← 함수의 가중치를 임의의 값으로 초기화\n",
    "\n",
    "w = np.random.rand(len(agent.action),env.reward.shape[0])\n",
    "w -= 0.5\n",
    "\n",
    "FA_Q_table = np.zeros((env.reward.shape[0],env.reward.shape[1],len(agent.action)))\n",
    "# 함수를 테이블에 저장\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        for k in range(len(agent.action)):\n",
    "            FA_Q_table[i,j,k] = w[k,0]  + w[k,1] * i + w[k,2]* j          \n",
    "\n",
    "# 학습된 정책에서 최적 행동 추출\n",
    "optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(FA_Q_table[i,j,:])\n",
    "\n",
    "\n",
    "print(\"Before : Function Approximation Q-learning : Q(s,a|w)\")\n",
    "show_q_table(np.round(FA_Q_table,2),env)\n",
    "print()\n",
    "print(\"Before : Function Approximation Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)  \n",
    "print()\n",
    "print(\"Initial w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "print()\n",
    "\n",
    "\n",
    "max_episode = 100000\n",
    "max_step = 100\n",
    "\n",
    "print(\"start Function Approximation : Q-learning\")\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "#각 에피소드에 대해 반복 :\n",
    "for epi in tqdm(range(max_episode)):\n",
    "    # s 를 초기화\n",
    "    i = 0\n",
    "    j = 0\n",
    "    agent.set_pos([i,j])\n",
    "\n",
    "    # 에피소드의 각 스텝에 대해 반복 :\n",
    "    for k in range(max_step):\n",
    "        pos = agent.get_pos()\n",
    "        # s에서 Behavior policy로 행동 a를 선택 (Gibbs 소프트맥스 함수사용)\n",
    "        action = np.zeros(4)\n",
    "        for act in range(len(agent.action)):\n",
    "            action[act] = w[act,0]  + w[act,1]* pos[0] + w[act,2]*pos[1]\n",
    "            \n",
    "        pr = np.zeros(4)\n",
    "        for i in range(len(agent.action)):\n",
    "            pr[i] = np.exp(action[i])/np.sum(np.exp(action[:]))\n",
    "                \n",
    "        action = np.random.choice(range(0,len(agent.action)), p=pr)  \n",
    "\n",
    "        # 행동 a 를 취한 후 보상 r과 다음 상태 s’를 관측\n",
    "        observation, reward, done = env.move(agent,action)\n",
    "        \n",
    "        # s’ 에서 Target policy 행동 a'를 선택 (𝑔𝑟𝑒𝑒𝑑𝑦)\n",
    "        next_act = np.zeros(4)\n",
    "        for act in range(len(agent.action)):\n",
    "            next_act[act] = np.dot(w[act,1:],observation) + w[act,0]\n",
    "        best_action = np.argmax(next_act)\n",
    "        \n",
    "        now_q = np.dot(w[action,1:],pos) + w[action,0]\n",
    "        next_q = np.dot(w[best_action,1:],observation) + w[best_action,0]\n",
    "       \n",
    "        # w 갱신\n",
    "        w[action,0] += alpha * (reward + gamma * next_q - now_q)\n",
    "        w[action,1] += alpha * (reward + gamma * next_q - now_q) * pos[0]\n",
    "        w[action,2] += alpha * (reward + gamma * next_q - now_q) * pos[1]\n",
    "        \n",
    "        # s가 마지막 상태라면 종료\n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "# FA_Q = np.zeros((len(agent.action),env.reward.shape[0],env.reward.shape[1]))\n",
    "# 함수를 테이블에 저장\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        for k in range(len(agent.action)):\n",
    "            FA_Q_table[i,j,k] = w[k,0]  + w[k,1] * i + w[k,2]* j          \n",
    "\n",
    "# 학습된 정책에서 최적 행동 추출\n",
    "# optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n",
    "for i in range(env.reward.shape[0]):\n",
    "    for j in range(env.reward.shape[1]):\n",
    "        optimal_policy[i,j] = np.argmax(FA_Q_table[i,j,:])\n",
    "\n",
    "    \n",
    "print(\"After : Function Approximation Q-learning : Q(s,a|w)\")\n",
    "show_q_table(np.round(FA_Q_table,2),env)\n",
    "print()\n",
    "print(\"After : Function Approximation Q-learning :optimal policy\")\n",
    "show_policy(optimal_policy,env)  \n",
    "print()\n",
    "print(\"Final w\")\n",
    "print(\"w = {}\".format(np.round(w,2)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}